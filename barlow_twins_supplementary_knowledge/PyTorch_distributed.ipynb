{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2dfcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14cc35",
   "metadata": {},
   "source": [
    "**SOURCE**\n",
    "\n",
    "https://theaisummer.com/distributed-training-pytorch/\n",
    "여기 보고함!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905255e",
   "metadata": {},
   "source": [
    "look at https://www.youtube.com/watch?v=3XUG7cjte2U&t=1832s\n",
    "\n",
    "https://www.youtube.com/watch?v=TibQO_xv1zc&t=2128s  ==> better\n",
    "\n",
    "https://www.youtube.com/watch?v=RQfK_ViGzH0&t=110s ==> 이거는 어려워서 필요없을수도\n",
    "\n",
    "\n",
    "제일 중요한 tutorial (이것보고 밑에 작성) : \n",
    "https://tutorials.pytorch.kr/beginner/dist_overview.html\n",
    "\n",
    "(github version of this : https://github.com/pytorch/tutorials/blob/master/beginner_source/dist_overview.rst) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97bfba",
   "metadata": {},
   "source": [
    "# Prelude. Basic Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0f4b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.device_count() #see how many gpus I can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef173b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c9d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8120fbca",
   "metadata": {},
   "source": [
    "# 그냥 https://theaisummer.com/distributed-training-pytorch/ 여기보고 따라하기!! 여기서 너무 설명 잘해줘서. 이걸로 1. 2.둘다 하면 될듯?\n",
    "(with https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51 and pytorch tutorial as theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f127d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d07b6ff",
   "metadata": {},
   "source": [
    "# 0. single GPU, DP 공통 샘플 코드\n",
    "우리는 위의 AI summer (https://theaisummer.com/distributed-training-pytorch/)  것을 써서 할 것이다. 따라서, DP, DDP 둘다 공통으로 사용하는 코드를 쓸 것이다\n",
    "\n",
    "https://github.com/The-AI-Summer/pytorch-ddp\n",
    "위에 blog의 코드들이 있다!\n",
    "\n",
    "\n",
    "# 아니다! 밑의 코드는 single GPU, DP만 공통이고, DDP 는 조금 다르다 ==> 이것은 그래서 chapter numbering을 조금 다르게 해야할듯?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66fbe4",
   "metadata": {},
   "source": [
    "# 1. Single GPU (즉, default)\n",
    "(사실, DP 도 여기코드를 거의 그대로 쓴다 (뒤에 보면 나올 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94824c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5e9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataloader\n",
    "def create_data_loader_cifar10():\n",
    "    transform = transforms.Compose([\n",
    "       transforms.RandomCrop(32),\n",
    "       transforms.RandomHorizontalFlip(),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    batch_size = 256 #8192\n",
    "    \n",
    "    #dataset들 정의 (trainset, testset) from pre-existing CIFAR10\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    \n",
    "    #dataloader들 정의 (train_loader, test_loader)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=10, pin_memory=True)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=10)\n",
    "    return trainloader, testloader \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d5c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop 하는 것을 만들기\n",
    "def train(net, trainloader):\n",
    "    print(\"started training ...\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = optim.SGD(net.parameters(),lr = 0.001, momentum = 0.9)\n",
    "    epochs = 1 #i.e. only one epoch 만 할것\n",
    "    num_of_batches = len(trainloader) #number of batches\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            #get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            images, labels = inputs.cuda(), labels.cuda() #move to cuda\n",
    "            \n",
    "            #zero the parameter gardients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #forward + backward + optimize\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print statistics\n",
    "            running_loss += loss.item() #.item because detach grad\n",
    "            \n",
    "        \n",
    "        print(f'[Epoch {epoch + 1}/{epochs}] loss: {running_loss / num_of_batches:.3f}')\n",
    "    print(\"Finished Traning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b9e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing loop 만들기\n",
    "def test(net, PATH,testloader): #PATH : 어디있는 state_dict를 load할지 \n",
    "    net.load_state_dict(torch.load(PATH)) #load the state_dict \n",
    "    \n",
    "    \n",
    "    correct = 0 #correct counter\n",
    "    total = 0 #total counter\n",
    "    \n",
    "    #since not training, set no_grad\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            \n",
    "            images, labels = images.cuda(), labels.cuda() #move to gpu\n",
    "            \n",
    "            #calculate output\n",
    "            outputs = net(images)\n",
    "            \n",
    "            #get loss value and update the number of correct/total \n",
    "            _, predicted = torch.max(outputs.data, 1) #maximum 인 것이 나의 prediction\n",
    "            total += labels.size(0) #256개의 batch 여서 256\n",
    "                    #i.e. torch.size([256]) => 256 으로 빼내기 (256: batch size btw)\n",
    "            correct += (predicted == labels).sum().item() #i.e. array같은 bool array, sum, then itemize\n",
    "            \n",
    "        acc = 100 * correct//total #// : floor division\n",
    "        print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a460ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connectome/dyhan316/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training ...\n",
      "[Epoch 1/1] loss: 5.933\n",
      "Finished Traning\n",
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "Total elapsed time: 33.02 seconds, \\  Train 1 epoch 20.13 seconds\n"
     ]
    }
   ],
   "source": [
    "#실제로 돌리기 (시간 많이 걸리니 주의!)\n",
    "start = time.time() \n",
    "\n",
    "PATH = './cifar_net_single_gpu.pth' #single gpu 의 결과 save pth (save path) \n",
    "trainloader, testloader = create_data_loader_cifar10()\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained = False).cuda() #이미 있는 model 가져오기\n",
    "\n",
    "start_train = time.time()\n",
    "\n",
    "#run training loop\n",
    "train(net, trainloader)\n",
    "\n",
    "end_train = time.time() \n",
    "\n",
    "#save \n",
    "torch.save(net.state_dict(), PATH) #PATH에 state_dict저장해놓기\n",
    "\n",
    "#test\n",
    "test(net, PATH, testloader) #path : 어디에있는 것을 불러올지 말함 \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "seconds = (end - start)\n",
    "seconds_train = (end_train - start_train)\n",
    "\n",
    "print(f\"Total elapsed time: {seconds:.2f} seconds, \\  Train 1 epoch {seconds_train:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882eddf0",
   "metadata": {},
   "source": [
    "# 2. DP (DataParllel) 로 여러개 돌리기\n",
    "(still slower than DDP)\n",
    "\n",
    "* batch size is divided across the number of workers\n",
    "* only a few changes to code needed\n",
    "\n",
    "ONLY ONE CHANGE IN CODE NEEDED! \n",
    "\n",
    "> ```net = nn.DataParallel(net)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f961ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "We will use : 2 gpus\n",
      "started training ...\n",
      "[Epoch 1/1] loss: 5.911\n",
      "Finished Traning\n",
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "Total elapsed time: 24.02 seconds, \\ Train 1 epoch 17.75 seconds\n"
     ]
    }
   ],
   "source": [
    "#net 정의하는 파트만 바뀌면 된다!\n",
    "#실제로 돌리기 (시간 많이 걸리니 주의!)\n",
    "start = time.time() \n",
    "\n",
    "PATH = './cifar_net_dp.pth' #single gpu 의 결과 save pth (save path) \n",
    "trainloader, testloader = create_data_loader_cifar10()\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained = False).cuda() #이미 있는 model 가져오기\n",
    "\n",
    "#######THE PART THAT WAS CHANGED######\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "    print(\"We will use : {} gpus\".format(torch.cuda.device_count()))\n",
    "\n",
    "start_train = time.time()\n",
    "\n",
    "#run training loop\n",
    "train(net,trainloader)\n",
    "\n",
    "end_train = time.time() \n",
    "\n",
    "#save \n",
    "torch.save(net.state_dict(), PATH) #PATH에 state_dict저장해놓기\n",
    "\n",
    "#test\n",
    "test(net, PATH, testloader) #path : 어디에있는 것을 불러올지 말함 \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "seconds = (end - start)\n",
    "seconds_train = (end_train - start_train)\n",
    "\n",
    "print(f\"Total elapsed time: {seconds:.2f} seconds, \\ Train 1 epoch {seconds_train:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8cd5a",
   "metadata": {},
   "source": [
    "**위에 돌려보면**, `nvidia-smi` 에서 볼 수 있듯이, gpu하나가 아닌 두개를 쓰게 된다!\n",
    "\n",
    "단, 여전히 그냥 하나만 쓰는 것보다는 느리다! \n",
    "\n",
    "**BUT** : batchsize= 256에서는 gpu 하나만 쓰는게 더 빠르지만, batchsize = 8192 에서는 DP가 더 빠름!\n",
    "\n",
    "* cpu/hard-disk bottleneck 때문에 그런 듯!\n",
    "    * 즉, batchsize가 작을때는, gpu하나에서만 돌려도 cuda core이 충분하기때문에, gpu를 두개로 늘림으로서 얻는 performance gain이 없다 (여전히 모든 batch가 parllel하게 돌아감)\n",
    "    * 하지만, batchsize가 커졌으면, DP의 경우 batch를 분산해서 보내기 때문에, increaed gpu 갯수로 인해서 증가하는 performance가 cpu/hard-disk bottleneck 로 인해 감소하는 performance보다 크다. 따라서, 더 빠른 듯?\n",
    "    \n",
    "**ASK IF THIS IS TRUE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619e460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c49cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb6d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "582480ef",
   "metadata": {},
   "source": [
    "# 3. DDP (DistributedDataParallel)로 더 efficiently 여러개 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41922c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a35e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b140b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ef4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713804f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5975adfa",
   "metadata": {},
   "source": [
    "# == 밑에 : torch tutorial을 하려고 했는데, 위에 aisummer이 훨씬 더 잘 해서, 그것을 쓰리고함.. 따라서 **밑의 것은 대부분 무시해도 될듯**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43ab8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e37ee4",
   "metadata": {},
   "source": [
    "# 1. use `DataParallel`\n",
    "lots of overhead so slow, but easy to implement \n",
    "\n",
    "looked at tutorial : https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c33651",
   "metadata": {},
   "source": [
    "## 1.0. setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96896e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#some parameters to use\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da62575a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ac85a",
   "metadata": {},
   "source": [
    "## 1.1. Dataset, Model등을 일반적으로 하듯이 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "413e2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset 임의로 만들기\n",
    "class dummy_dataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size) #임의의 데이터 (of shape length x size) 를 만들어내기 \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len #즉, 그냥 받아오도록 하기 \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "#그걸로 임의로 dataloader만들어서 intialize시키기\n",
    "dummy_loader = DataLoader(dataset = dummy_dataset(size = input_size, length=data_size),\n",
    "                        batch_size = batch_size, shuffle = True)\n",
    "\n",
    "##testing it out\n",
    "#print(dummy_loader)\n",
    "#print(next(iter(dummy_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9483c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model임의로 만들기\n",
    "class Model(nn.Module): \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(self.input_size, self.output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.fc(input) #single layer NN\n",
    "        print('\\tIn MOdel : input size', input.size(),\n",
    "             'output_size', output.size())\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e4faa",
   "metadata": {},
   "source": [
    "## ?X?X?X?X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d096aeaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (979568611.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [13]\u001b[0;36m\u001b[0m\n\u001b[0;31m    if torch.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_size, output_size)\n",
    "\n",
    "if torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d08ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68d2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa79dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e83e49",
   "metadata": {},
   "source": [
    "#### torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfcf6a",
   "metadata": {},
   "source": [
    "## 2. DDP\n",
    "https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51\n",
    "\n",
    "위에 랑\n",
    "\n",
    "https://theaisummer.com/distributed-training-pytorch/\n",
    "여기 보기!!\n",
    "(torch documentation/tutorial 보다 잘 되어있는 듯?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd7e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
