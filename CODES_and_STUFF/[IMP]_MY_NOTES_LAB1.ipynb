{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1UUWPWwZeRFQ3-YLL5SD3XdFD5qI-HkSU?usp=sharing \n",
    "내가 저번에 정리했던 것인데, 이것보다 훨씬 자세히 (but convoluted)하게 적혀있다! 이것다 보고이해하면 위의 링크타고가서 보기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 집 연장하기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "1. Tensor Library for DL using GPU&CPU\n",
    "\n",
    "2. Autograd Engine\n",
    "    * does backprop automatically\n",
    "    * (do this by) storing gradietns for all tensors in networks and using them to do backprop\n",
    "    \n",
    "3. Define-by-run (cf : Define-and-run with Tensorflow)\n",
    "    * dynamic network dynamically, more model flexibility (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Attributes\n",
    "* `shape`\n",
    "    * `unsqueeze`, `squeeze` : (`(2) <-> (1,2)`)\n",
    "    * `reshape` : (`(4)-> (2,2)`)\n",
    "    * `permute` : (`(3,1,4) -> (4,1,3)`) (dimension swap)\n",
    "* `dtype`\n",
    "* `device` => which device the tensor is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.arange(8).reshape(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7]])\n",
      "torch.Size([4, 2])\n",
      "cpu\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "print(t1.shape)\n",
    "print(t1.device)\n",
    "print(t1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "t1_mps = t1.to(device)\n",
    "print(t1_mps.device)\n",
    "print(t1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====unsqueeze=====\n",
      "t2 : \n",
      " tensor([0.7166, 0.5125]) torch.Size([2])\n",
      "t2.unsqueeze(0): \n",
      " tensor([[0.7166, 0.5125]]) torch.Size([1, 2])\n",
      "t2.unsqueeze(1): \n",
      " tensor([[0.7166],\n",
      "        [0.5125]]) torch.Size([2, 1])\n",
      "====squeeze=====\n",
      "t3 : \n",
      " tensor([[[0.2264],\n",
      "         [0.0173]]]) torch.Size([1, 2, 1])\n",
      "t3.squeeze(0) :\n",
      " tensor([[0.2264],\n",
      "        [0.0173]]) torch.Size([2, 1])\n",
      "t3.squeeze(1) :\n",
      " tensor([[[0.2264],\n",
      "         [0.0173]]]) torch.Size([1, 2, 1])\n",
      "t3.squeeze(2) :\n",
      " tensor([[0.2264, 0.0173]]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"====unsqueeze=====\")\n",
    "#unsqueeze\n",
    "t2 = torch.rand(2)\n",
    "\n",
    "print(\"t2 : \\n\", t2, t2.shape)\n",
    "print(\"t2.unsqueeze(0): \\n\", t2.unsqueeze(0),t2.unsqueeze(0).shape)\n",
    "print(\"t2.unsqueeze(1): \\n\", t2.unsqueeze(1),t2.unsqueeze(1).shape)\n",
    "\n",
    "#squeeze\n",
    "print(\"====squeeze=====\")\n",
    "t3 = torch.rand(1,2,1)\n",
    "print(\"t3 : \\n\", t3, t3.shape)\n",
    "print(\"t3.squeeze(0) :\\n\", t3.squeeze(0),t3.squeeze(0).shape)\n",
    "print(\"t3.squeeze(1) :\\n\", t3.squeeze(1),t3.squeeze(1).shape)\n",
    "print(\"t3.squeeze(2) :\\n\", t3.squeeze(2),t3.squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====reshape & permute======\n",
      "t4 : \n",
      " tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]]]) torch.Size([2, 2, 4])\n",
      "t4.reshape(2,4,2) : \n",
      " tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11],\n",
      "         [12, 13],\n",
      "         [14, 15]]]) torch.Size([2, 4, 2])\n",
      "t4.permute(2,1,0) : \n",
      " tensor([[[ 0,  4],\n",
      "         [ 8, 12]],\n",
      "\n",
      "        [[ 1,  5],\n",
      "         [ 9, 13]],\n",
      "\n",
      "        [[ 2,  6],\n",
      "         [10, 14]],\n",
      "\n",
      "        [[ 3,  7],\n",
      "         [11, 15]]]) torch.Size([4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"====reshape & permute======\")\n",
    "t4 = torch.arange(16).reshape(2,2,4)\n",
    "\n",
    "print(\"t4 : \\n\", t4, t4.shape)\n",
    "print(\"t4.reshape(2,4,2) : \\n\", t4.reshape(2,4,2), t4.reshape(2,4,2).shape)\n",
    "print(\"t4.permute(2,1,0) : \\n\", t4.permute(2,0,1), t4.permute(2,1,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "#Tensor attribu†e : device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#moving the M1 GPU instead\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "t2_gpu = t2.to(device)\n",
    "print(t2.device) #t2 : in cpu\n",
    "print(t2_gpu.device) #t2 : in gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Tensor Operations\n",
    "\n",
    "1. Basic Operations\n",
    "2. Dimension Reduction\n",
    "3. Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====basic setup=====\n",
      "t1 : \n",
      " tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "t2 : \n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "=====convert numpy -> tensor====\n",
      "<class 'numpy.ndarray'> \n",
      " [[0 1]\n",
      " [2 3]]\n",
      "<class 'torch.Tensor'> \n",
      " tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "=====element-wise operations====\n",
      "t1+torch.exp(t2)+3 :  \n",
      " tensor([[5.7183, 6.7183],\n",
      "        [7.7183, 8.7183]])\n",
      "=====broadcasting====\n",
      "vector : \n",
      " tensor([1, 2])\n",
      "t1+vector :  \n",
      " tensor([[1, 3],\n",
      "        [3, 5]])\n"
     ]
    }
   ],
   "source": [
    "print(\"====basic setup=====\")\n",
    "t1 = torch.arange(4).reshape(2,2)\n",
    "t2 = torch.ones(2,2)\n",
    "print(\"t1 : \\n\", t1)\n",
    "print(\"t2 : \\n\", t2)\n",
    "\n",
    "\n",
    "print(\"=====convert numpy -> tensor====\")\n",
    "num_array = np.arange(4).reshape(2,2)\n",
    "torch_tensor = torch.tensor(num_array)\n",
    "\n",
    "print(type(num_array), '\\n', num_array)\n",
    "print(type(torch_tensor),'\\n', torch_tensor)\n",
    "\n",
    "\n",
    "print(\"=====element-wise operations====\")\n",
    "print(\"t1+torch.exp(t2)+3 :  \\n\",t1+torch.exp(t2)+3)\n",
    "\n",
    "\n",
    "print(\"=====broadcasting====\")\n",
    "vector = torch.tensor([1,2])\n",
    "print(\"vector : \\n\", vector)\n",
    "print(\"t1+vector :  \\n\",t1+vector)\n",
    "#if error : braodcasting 관련 documentatino 보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====show indices of max/min (argmax, argmin)====\n",
      "t1 : \n",
      " tensor([[-0.0515, -0.1822],\n",
      "        [-0.0233,  0.0018],\n",
      "        [-0.6719,  1.2701]])\n",
      "t1.argmax(axis=0) : \n",
      " tensor([1, 2])\n",
      "t1.argmax(axis=1) : \n",
      " tensor([0, 1, 1])\n",
      "t1.argmax() : \n",
      " tensor(5)\n",
      "======comapre for : when we keep dimensions ============\n",
      "t1.argmax(axis=1, keepdims) : \n",
      " tensor([[0],\n",
      "        [1],\n",
      "        [1]]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"=====show indices of max/min (argmax, argmin)====\")\n",
    "import random\n",
    "random.seed(3)\n",
    "t1 = torch.randn(3,2)\n",
    "print(\"t1 : \\n\",t1)\n",
    "print(\"t1.argmax(axis=0) : \\n\",t1.argmax(axis=0))\n",
    "print(\"t1.argmax(axis=1) : \\n\",t1.argmax(axis=1))\n",
    "print(\"t1.argmax() : \\n\",t1.argmax())\n",
    "\n",
    "print(\"======comapre for : when we keep dimensions ============\")\n",
    "print(\"t1.argmax(axis=1, keepdims) : \\n\",t1.argmax(axis=1, keepdims = True),\n",
    "     t1.argmax(axis=1, keepdims = True).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data\n",
    "\n",
    "* `torch.utils.data.Dataset` \n",
    "    * a python class defining a **Dataset**\n",
    "    * called by `Dataloader` implicitly (dataloader을 쓰려면 이것이 필요)\n",
    "    * transformation을 input으로 받아, 어떤 transformation을 행할지 정해주기도 함\n",
    "    * custom Dataset class을 정의하고 싶다면, need to **MUST AT LEAST implement the following methods**\n",
    "        * `__init__` : **initialize dir/file** to read images, transform \n",
    "        * `__getitem__` : **loads data** from dataset **at a given index** (ex : `XXX[0]`)\n",
    "        * `__len__` : returns size of the dataset (ex : len(XXX))\n",
    "\n",
    "<br> \n",
    "\n",
    "* `torch.utils.dataset.Dataloader`\n",
    "    * Sends dataset to models\n",
    "    * 이것을 쓰려면, 앞에서 말한 `Dataset`이 정의가 되어있어야 한다 (그래야 dataset을 가져와서 Dataloader이 model에 data 를 loading하니)\n",
    "    * Paremeters\n",
    "        * `batch_size` (default = 1)\n",
    "        * `shuffle` (shuffle data for training)\n",
    "        * `num_workers` (병렬연산때)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Custom Dataset 만들자 (using dummy random data)\n",
    "\n",
    "위에서 말했듯이 3개의 methods가 무조건 들어가야하는데 그 셋이 쓰이는 방법이 조금씩 다름 (이거 왜 그런지 **ASK** (배웠던 것 같은데...?))\n",
    "* `__init__` : should not do `return` \n",
    "* `__getitem__` : `dataset[3]` 이런식으로 call 한다\n",
    "* `__len__` : `len(dataset)` 이런식으로 쓴다!\n",
    "(사용예시 밑에 있다)\n",
    "\n",
    "# ASK :         #여기서는 아무것도 return 하면 안됨! (return sth sth 하면 에러뜸! (__init__ 의 condition인듯?)) => ASK WHY\n",
    "-> __init__ 이랑 __call__(getitem) __len__ 이런식으로 쓴다 \n",
    "미리 Dataset이라고 된 곳에서 inherit하는 것이다 (거기있던 init, len, 을 가져오는 것) => so that we don't have to write everything (애초에 그렇게 설계가 되어있다) (거기다가 덮어서 쓸 수 있게)\n",
    "\n",
    "그래서 call, len, init은 실행되는 것은 실행이 되는 시간이 정해져있다\n",
    "(init : object를 만들떄 (instantiate)그 시점에 __init__이 실행되는 것)\n",
    "(set attributes and stuff)\n",
    "\n",
    "getitem : index 값 하나**만** 받을 수 있는 것 (list 도 일종의 class인데, list[2] : possible because list has the __getitem__ attribute\n",
    "\n",
    "dataset 에서는 하나의 index 에 대해서 받아오는 것만 하고, index 여러개를 가져와야 한다면 dataloader에서 지정을 해준다 \n",
    "\n",
    "\n",
    "==> 그래서 dataloader 이 model에 들어가는 것(dataset은 custom, dataloader은 거의 custom을 안한다)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Define dummy dataset#####\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset): #must pass the `Dataset` as a parent class (to inherit its stuff)\n",
    "    ###now let's define the three methods that was said to be \"MUST\" (init, getitem, len)\n",
    "    \n",
    "    def __init__(self): #here, the dataset is defined (usually by directory, but here literally made)\n",
    "        n_dataset = 100\n",
    "        \n",
    "        #set random x data\n",
    "        self.x_data = [] #initialize\n",
    "        for _ in range(n_dataset): #make list with n_dataset # of samples (items)\n",
    "            random_length = random.randint(5,10) #random length of each sample's x length\n",
    "            self.x_data.append(np.random.randint(0,255,random_length)) #create random array and append\n",
    "        \n",
    "        #set random y data\n",
    "        self.y_data = torch.rand(n_dataset, 1)\n",
    "        \n",
    "        print(\"SimpleDataset setting done\")\n",
    "        print(f'x_data shape: {len(self.x_data)}')\n",
    "        print(f'x_data random length (1st, 4th, 46th): {len(self.x_data[0]),len(self.x_data[3]), len(self.x_data[45])}')\n",
    "        print(f'y_data shape: {self.y_data.shape}')\n",
    "        \n",
    "        #여기서는 아무것도 return 하면 안됨! (return sth sth 하면 에러뜸! (__init__ 의 condition인듯?))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.x_data) # returns the length of the x_data (i.e. number of samples)\n",
    "    \n",
    "    def __getitem__(self, idx): #gets idx(index) as an input\n",
    "        x = torch.FloatTensor(self.x_data[idx]) #i.e. get the ith sample's x data\n",
    "        y = torch.FloatTensor(self.y_data[idx]) #i.e. get the ith sample's y data\n",
    "        return x, y\n",
    "\n",
    "    ####custom method definintion###\n",
    "    #because the length of individual samples is not unifor, we must pad zeros to the right if there's some missing \n",
    "    #(for unifiorm batch input dimensions)\n",
    "    \n",
    "    def collate_fn(self, data): #gets data as input (구체적인 작동원리는 공부하지 말고 일단은 생략하자 )\n",
    "        max_len = 10 #maximum length\n",
    "        batch = [] #initial batch size\n",
    "        \n",
    "        for x,y in data: #i.e. data is XXXXXx\n",
    "            x_padded = torch.cat([x, torch.zeros(max_len - x.shape[0])]) #i.e. 부족한 dim만큼 0으로 padding\n",
    "            batch.append(x_padded) #i.e. now append the padded x to the batch      \n",
    "        return torch.stack(batch) #now stack the batch `list` into an `tensor`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====first instantiating the dataset class====\n",
      "SimpleDataset setting done\n",
      "x_data shape: 100\n",
      "x_data random length (1st, 4th, 46th): (6, 6, 8)\n",
      "y_data shape: torch.Size([100, 1])\n",
      "====now lets try the datset methods====\n",
      "custom_data :  <__main__.CustomDataset object at 0x7f92a8f6eac0>\n",
      "custom_data.__init__ :  <bound method CustomDataset.__init__ of <__main__.CustomDataset object at 0x7f92a8f6eac0>>\n",
      "len(custom_data) :  100\n",
      "custom_data[10] :  (tensor([216.,  59., 161., 113.,  57.,  19.,   2.,  58.,  27.]), tensor([0.7552]))\n"
     ]
    }
   ],
   "source": [
    "#initialize our dataset and stuff!\n",
    "\n",
    "print(\"====first instantiating the dataset class====\")\n",
    "custom_data = CustomDataset() #initialize the custom_dataset class\n",
    "\n",
    "print(\"====now lets try the datset methods====\")\n",
    "print(\"custom_data : \",custom_data)\n",
    "print(\"custom_data.__init__ : \", custom_data.__init__) #as expected, doesn't return anything\n",
    "print(\"len(custom_data) : \", len(custom_data))  #we can use `len` as if it's built-in! (forgot what it's called)\n",
    "                                                #print(custom_data.__len__()) #option2, more obsecure method\n",
    "print(\"custom_data[10] : \", custom_data[10]) #이런식으로 `__getitem__`했던 것 사용 가능!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Custom Dataloader 만들자 (using cusotm Dataset class we made)\n",
    "\n",
    "We will use `DataLoader` class, which was already imported before\n",
    "some optoins we can set : \n",
    "* `shuffle`\n",
    "* `batch_size`\n",
    "* `collate_fn` \n",
    "* $\\cdots$ (생략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f92a8f6ea30>\n",
      "batch_idx : 0 , sample.shape : torch.Size([25, 10]) \n",
      "batch_idx : 1 , sample.shape : torch.Size([25, 10]) \n",
      "batch_idx : 2 , sample.shape : torch.Size([25, 10]) \n",
      "batch_idx : 3 , sample.shape : torch.Size([25, 10]) \n",
      "last batch example : \n",
      " tensor([[  5., 220.,  44.,  38.,  20., 188.,  71.,  14., 120.,  32.],\n",
      "        [116.,  81., 152.,   0., 237., 166., 203.,   0.,   0.,   0.],\n",
      "        [201.,  50.,  70., 101., 223.,  95., 128., 223.,   0.,   0.],\n",
      "        [164.,  50.,  13.,  68., 160.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [135.,   8.,  84., 198., 253.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [216.,  59., 161., 113.,  57.,  19.,   2.,  58.,  27.,   0.],\n",
      "        [ 66., 200., 137., 231.,  69.,  22., 145.,  42.,  92.,   0.],\n",
      "        [105.,  87., 184., 167., 239.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [233., 188.,  72.,  13., 183.,  12.,  22., 252.,   0.,   0.],\n",
      "        [ 10., 237., 109.,  60.,  45., 201.,  83.,  82., 149., 191.],\n",
      "        [ 10., 181., 155.,  91., 241., 104.,  17.,  46.,   0.,   0.],\n",
      "        [231., 162., 228.,  30.,  76., 131.,  57., 152.,  32.,   0.],\n",
      "        [238., 164., 221.,  13., 221., 172.,  49.,   0.,   0.,   0.],\n",
      "        [159.,  50.,  90.,  10.,  25.,  17.,  65.,  58., 225., 216.],\n",
      "        [ 64.,   2., 179., 194., 125.,   7., 112.,  59.,   0.,   0.],\n",
      "        [118.,  22.,  85., 250.,  70., 222., 217.,  65., 133.,   0.],\n",
      "        [  5., 107., 136., 205.,  65., 182.,   5.,   0.,   0.,   0.],\n",
      "        [182., 152., 173.,  90., 187., 236.,  71.,  71., 103.,   0.],\n",
      "        [248., 202., 160., 164.,  66., 233.,   0.,   0.,   0.,   0.],\n",
      "        [205.,  65., 190., 203., 157., 120.,  97.,   0.,   0.,   0.],\n",
      "        [  9.,  50.,  79.,   0., 252., 110., 252.,  99.,   0.,   0.],\n",
      "        [ 30., 228.,  73., 101.,  30., 228.,   0.,   0.,   0.,   0.],\n",
      "        [ 67.,  62., 182.,  67., 162., 198., 211., 234., 110.,   0.],\n",
      "        [101., 198., 154., 152., 235., 152., 101.,   0.,   0.,   0.],\n",
      "        [111., 136., 161., 214.,  47., 191., 166., 186., 225.,   0.]])\n"
     ]
    }
   ],
   "source": [
    "#create dataloder isntance\n",
    "#here, batch_size , shuffle 등등 : 이미 Torch의 dataloader에 구현되어있는 것들이어서 class input으로만 넣어주면 끝!\n",
    "\n",
    "custom_dataloader = DataLoader(custom_data , batch_size = 25, shuffle = True,  \n",
    "                               collate_fn = custom_data.collate_fn, ) #instantiate Dataloader class with (an instance of) thecustom dataset \n",
    "\n",
    "print(custom_dataloader)\n",
    "\n",
    "#왜 밑에서처럼 되는지 모르겠으나 일단은 간단하게 보는 거니 이유 보고 따지는 것은 skip (무슨 iterable object etcetc 와 연관이 있을 듯)\n",
    "\n",
    "for batch_idx, samples in enumerate(custom_dataloader): #apparently, the object is enumerate-able\n",
    "    #viewing the batch idx and sample shapes\n",
    "    print(\"batch_idx : {} , sample.shape : {} \".format(batch_idx, samples.shape))\n",
    "    \n",
    "    #printing the last batch for example\n",
    "    if batch_idx == len(custom_dataloader) -1 : #i.e. when it's the last batch\n",
    "        print(\"last batch example : \\n\",samples) \n",
    "#print(custom_dataloader[0]) #apparently, not indexable...?\n",
    "#print(next(custom_dataloader)) #not an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Pre-loaded dataset\n",
    "\n",
    "참고로, 위에서 custom하게 dataset 정의해도 되지만, `Torchvision` 같은 데에서, `Dataset class inherit`가 이미된, dataset을 제공해주기도 한다! 밑에서 실제로 ML할때는 이것을 사용해서 하도록 하자! (will use MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ACTUALLY implementing a model with example dataset (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.1. Preparing DataSet, DataLoader (MNIST)\n",
    "이미 Dataset 으로 제공되는 MNIST를 가지고 해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor #to go from image to tensor I believe\n",
    "\n",
    "\n",
    "\n",
    "####1. instantiate DataSet!#####\n",
    "train_data = datasets.MNIST(\n",
    "    train = True, #the only thing different with test_data, specfieis that we want training dataset\n",
    "    root = \"data\",\n",
    "    download = True,\n",
    "    transform = ToTensor() #import as torch tensor\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    train = False, #the only thing different with test_data, specfieis that we want training dataset\n",
    "    root = \"data\",\n",
    "    download = True,\n",
    "    transform = ToTensor() #import as torch tensor\n",
    ")\n",
    "\n",
    "\n",
    "####2. instantiate DataLoader (using previously defined dataset)#####\n",
    "batch_size_to_use = 64 #arguments for DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size_to_use , shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size= batch_size_to_use, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example data shape :  torch.Size([64, 1, 28, 28])\n",
      "example target shape :  torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9279b9ff40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###looking at one example###\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##왜 이렇게 작동하는지 지금은 잘 모르겠다... for now, just roll with it\n",
    "examples = enumerate(test_loader) \n",
    "\n",
    "batch_idx , (example_data, example_targets) = next(examples)\n",
    "\n",
    "print(\"example data shape : \", example_data.shape) #[64,1,28,28] : batch size of 64, color of 1, 28*28 image (I think)\n",
    "print(\"example target shape : \", example_targets.shape)\n",
    "\n",
    "#look at first sample of the batch \n",
    "plt.imshow(example_data[0].squeeze()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Building Model\n",
    "\n",
    "use `torch.nn` module's `nn.Module`!!! and use inheritance to create an instance! (`nn`은 module이고, `nn.Module`이 class 라서 그런듯?)\n",
    "\n",
    " **REQUIREMENTS WHEN MAKING A MODEL**\n",
    " * inherit from **`nn.Module`** (NOT `nn`!!!)\n",
    " * `__init__` 가 있어야함\n",
    "     * `super(custom_NN, self).__init__()` 가 되어있어야 한다! (probably to bring attributes)\n",
    "     * 여기서 보통, `self.fc1 = nn.Linear(28*28, 128)` 이런식으로, forward할때 쓰일 것들을 attribute로 정의함\n",
    "         * 아마도 `nn.Linear`로 하는 이유가, 이렇게 하면 자동으로 gradient tracking을 해서 나중에 Autograd할때 쓰이기 때문일듯..? (ASK)\n",
    " * `__forward__` 가 있어야함 (for forward-propagation)\n",
    "     * use `__init__`에서 정의한 stuff to make \n",
    "     * use `F.relu` and so on to make nonlinear layer\n",
    " \n",
    " \n",
    "<br>\n",
    "\n",
    "**some things that are used**\n",
    "* `torch.nn.functional as F` are frequently used \n",
    "    * ex : `F.relu`, `F.sigmoid`\n",
    "\n",
    "<br>\n",
    "again, \n",
    "\n",
    "* `nn` : module\n",
    "\n",
    "* `nn.Module` : base class for all NN modules \n",
    "\n",
    "$\\therefore$, custom class를 만들때는 `nn`이 아닌 `nn.Module`에서 가져와야함!\n",
    "\n",
    "# **QnA**\n",
    "Q1. 밑에서 forward할떄, `x = x.reshape(-1, 784)`, 즉 (shape : [1,784])을 쓰는데, why not use shape [784]??\n",
    "* A : 왜냐하면 torch 에서는 shape가 `[(batch size), (input vector size)]`로 되어있어야 해서!!!!!\n",
    "    * `nn.Linear`이런 함수들도 **모두 `[(batch size, (input vector)]` 꼴**의 input을 받도록 설계되어있다 \n",
    "    * 따라서, batchsize=1이어도 1을 꼭 넣어줘서 [1,784]로 해야한다\n",
    "        * `.reshape[-1,784]`는, `[1*784] -> [1,784]`, `[2,2*784] -> [4,784]` 등의 꼴로 만들어주는 역할! (즉, -1은 일종의 `magic` resahping)\n",
    "        * 따라서 `-1`을 reshaping에 쓰는 것은 batch size 상관없이 \n",
    "\n",
    "# **ASK** \n",
    "1. `x = x.reshape(-1, 784) #shape : (1,784) => ASK why not [784] 안쓰고 [1,784]?`\n",
    "A : nn.Linear : weight가 있는 것이다 그런데 nn.Linear에서의 약속은 ['batch dimension', 'weight에 곱하는 feature dimension']으로 정의를 했기 때문에, 무조건 첫번째 dimension은 batch 가 되도록 정해주는 것이다! (즉, batchsize=1이어도 1을 꼭 넣어줘서 [1,784] ) (-1의의미 : 임의의 어떤 값 (flexible하게 내가 주는 것에 따라서 달라진다)(모아서 update하는지 등등은 training loop 에서 지정한다 (-1은 어떤 batch size든 되도록 하기위해서 하는 것. batch size가 확실하면 지정해줄 수 는 있지만 not-robust)\n",
    "\n",
    "\n",
    "2. `super`이 하는 것은 어떤 attribute를 가져오는 것인가? 그냥 `class custom_NN(nn.module)` 이라고 할때, `nn.Module`이 이미 inherit된것이 아닌가? super 을 하면 어떻게 다른거지?\n",
    "A : `super` : nn.Module을 상속받으머ㅕㄴ method는 받으나, `__init__`은 자동으로 상속되지 앟는다 그래서 super을 해주는 것 (보통 그냥 넣어주면 안전)(__init__은 연산이라기 보다는 그냥 불러오는 것이어서 용량걱정 같은거는 안해도 될듯)(그런데 제품을 받을때는 어떤 method는 상속안되도록 capsule화를 시킨다. 그래서 self.__fc1 이렇게 하면 이것은 상속을 안받는 것으로 하기는 한다) \n",
    "A : 사실 __init__을 불러오는 것도 내 바로 위에있는 아이한테 상속받는다는 뜻이고, 여기서는 nn.Module을 지정했기에 super내에서 따로 지칭을 안해도 nn.Module을 가져온다\n",
    "=> 그런데 만약 nn.Module의 부모의 부모 등등도 받고싶다고하면 그런 방법들도 존재한다 (super 은 부모한테만 받아온다는 뜻)\n",
    "=> 상속자체는 주로 기본적으로 하나만 한다\n",
    "\n",
    "3. `nn.Module`에 없는 operation이 없으면 내가 직접 수학적인 수식을 코드로 표현해서 넣을 수 있는 가? 어떻게 해야하는가? 예시가 있는 가?\n",
    "=> nn.Linear안에 약속이 있는 것이었는데 그런식으로 layer을 지정해줄 수 있다 (custom operation) (nn.Linear source코드 비슷하게)\n",
    "\n",
    "4. 위에서 적은, `__init__`에서  `nn.Linear`로 하는 이유에 대해서 묻기 : is it really\n",
    "     * 아마도 `nn.Linear`로 하는 이유가, 이렇게 하면 자동으로 gradient tracking을 해서 나중에 Autograd할때 쓰이기 때문일듯..? (ASK)\n",
    "     \n",
    " \n",
    "5. `nn.Linear`을 `__init__`이 아닌 `forward`에서 써서 정의해도 되는가?\n",
    "=> 이러헥 하면 안된다! \n",
    "because :\n",
    "1. deeplearning : 고정된 weight를 변화시키는 건데, forward는 input이 들어올때마다 실행되ㅡㄴ 것이니 만약 거기서 nn.linaer을 하게되면 그때마다 layer 를 initialize하게 된다=> 따라서 이렇게 하면 triaing이 안된다\n",
    "=> 즉, self.fc2에 \"weight\"가 저장됨 \n",
    "\n",
    "=> 이러한 이유때문에, \n",
    "128 => 128 => 128 ->10 \n",
    "여기서 forward할때 self.fc2(x), self.fc2(x)두번을 하면...? => 같은 layer을 여러번 돌리는 것이라서 하면 안된다! => 이러려면 fc2,fc3이런시그올 해야함\n",
    "\n",
    "한번 backward를 하면 weight가 True가 된다 그래서 fc여러번 쓰는게 될지는 모르겠다 \n",
    "==> 아마도 에러날듯 (forward를 한 후에 나중에 autograd 할때 두번저장된 weight이니) => 그게 의도한 바가 아닐 수도 있다 \n",
    "\n",
    "\n",
    "만약 layer이 많다면 일일히 적어주기? => 방법은 많다\n",
    "1. nn.Sequential 쓰는 사람도 있고\n",
    "2. add하는 방법을 쓰는 방법도 있고 (dictioanry에다가 추가해서)(.save_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m16\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.arange(16).reshape(2,2,2,2)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "print(a.reshape(-1,16))\n",
    "print(a.reshape(-1,8))\n",
    "#i.e. it's inferred!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F #module that has lots of functions that are frequently used (ex : sigmoid, relu, etc)\n",
    "\n",
    "class custom_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_NN, self).__init__() # bring attributes from nn.Module into this 이것 없으면 안됨!\n",
    "        self.fc1 = nn.Linear(28*28, 128) #28*28 : sample size, 128 : hidden layer size\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.fc4_final = nn.Linear(64, 10) #10 : # of classes\n",
    "        \n",
    "    def forward(self, x): # x : input!\n",
    "        x = x.reshape(-1, 784) #shape : (1,784) => ASK why not [784] 안쓰고 [1,784]?\n",
    "        #x = x.flatten() #이렇게 하면 애러난다! (shape : (784)) => ask why\n",
    "        x = F.relu(self.fc1(x)) #use F.relu method (matrix multiplication then relu func)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        x = self.fc4_final(x) #final layer, don't apply nonlinear func, just linear combination\n",
    "        return x \n",
    "        #option 2 : nn.Sequential (look at my original collab notes)\n",
    "\n",
    "#instantiate the custom model!\n",
    "network = custom_NN()  # this will be used kinda like the \"model\" in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====custom_NN instance, network, 를 보자====\n",
      "network : \n",
      " custom_NN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4_final): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "network.parameters : \n",
      " <bound method Module.parameters of custom_NN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4_final): Linear(in_features=64, out_features=10, bias=True)\n",
      ")>\n",
      "=====samle data를 보자====\n",
      "sample_data.shape : \n",
      " torch.Size([1, 28, 28])\n",
      "=====forward pass possible! (because we did `def forward()` before)=====\n",
      "network(sample_data)(forward pass) : \n",
      " tensor([[-0.0561,  0.1124,  0.1760, -0.3703, -0.1068, -0.1431,  0.3149, -0.6038,\n",
      "         -0.1925,  0.4705]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunmi/opt/anaconda3/envs/ML_tutor/lib/python3.8/site-packages/torch/nn/functional.py:1959: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "##some observatino about the custom_NN class\n",
    "print(\"=====custom_NN instance, network, 를 보자====\")\n",
    "network = custom_NN() \n",
    "print(\"network : \\n\",network)\n",
    "print(\"network.parameters : \\n\",network.parameters) #used when defining Optim (for some reason)\n",
    "\n",
    "\n",
    "print(\"=====samle data를 보자====\")\n",
    "sample_data = example_data[0] \n",
    "print(\"sample_data.shape : \\n\", sample_data.shape)\n",
    "\n",
    "print(\"=====forward pass possible! (because we did `def forward()` before)=====\")\n",
    "print(\"network(sample_data)(forward pass) : \\n\", network(sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO FROM HERE!!\n",
    "\n",
    "We'll do \n",
    "\n",
    "1. Training Loop (`def train():`)\n",
    "2. Testing Loop (`def test():`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필수\n",
    "**Requirements when making a training loop**\n",
    "* `model.train()` : turn **training mode on**\n",
    "* **perform backprop/optimization** by following the steps:\n",
    "    * `optim.zero_grad()` (b/c gradient by default adds up, so have to initialize for every mini-batch)\n",
    "    * `output = model(input_tensor)` (i.e. forward pass once and get output)\n",
    "    * `loss = loss_fn(output)` (i.e. compute loss) => also automatically makes computational tree, which enables Autograd engine)(i think)\n",
    "        * 여기서 `loss` : just another torch TENSOR! (but with `requires_grad = True`)\n",
    "        * 즉, 사실 밑에서 하는 `loss.backward()` 는 사실 아무 torch tensor (with `requires_grad=True`에 해도 되는 것!)\n",
    "    * `loss.backward()` (compute gradient using Autograd engine)\n",
    "    * `optim.step()` (perform optimization (backprop, 1 step) using gradient obtained previously)\n",
    "* compute training loss (optional)\n",
    "* `model.to(device)` ,`input_tensor.to(device)`, ... : tensor, model등을 device로 보내기!\n",
    "* FINAL : perform training by calling `train(fjdklsjfklsdfl)` ...\n",
    "\n",
    "<br>\n",
    "\n",
    "common `train` loop function arguments (즉, 어떤 함수들을 가지고 어떤 방법으로 training할지도 밑에서 처럼 training loop의 argument로 들어가게 해서 조정할 수 있다!\n",
    "* what model to train (`model` in code below)\n",
    "* training data (`train_loader` in code below)\n",
    "* optimization method (`optim` in code below)\n",
    "* loss function definition (`loss_fn` in code below)\n",
    "* nubmer of epochs (`epochs` in code below)\n",
    "* CPU or GPU (`device` in code below)\n",
    "** 위의 `optim`, `loss_fn` 옵션들은 예시가 많아서 적지 않았으나, 밑의 코드 예시를 보면 대충 알 수 있다 (ex : `optim.SGD`, `nn.CrossEntropyLoss()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 할만한 것들 ####\n",
    "\n",
    "1. **`loss` is just another torch tensor with `requires_grad = True`!!**\n",
    "    * 이유는 notebook 맨 밑의 보충설명 (autograd engine and requires_grad and etc)를 보면 나오기는 하나, 간단하게 말하면 `loss` is just another tensor, and `.backward()` is an operation that can be applied to ANY tensor! (nn.Module을 inherit한 모델이 아니어도, computaitonal graph이기만 하면 된다) => notebook맨 밑에 그 예시가 나옴\n",
    "    * 따라서  `(any tensor).backward()`도 가능하다! (notebook 맨 밑에서는 실제로 그렇게 함)\n",
    "    \n",
    "=> loss라고 하는 것은 nn.Linear을 거치고 계속하면 tensor=> tensor ... => tensor이라서 loss의 형태도 tensor이다\n",
    "=> 대부분 prediction할때는 prediction 값 tensor랑 target 값 tensor이랑 이 둘간의 연산이 loss이기 때문에, 당연히 loss를 계산할때까지의 모든 것이 requires_grad로 되고, loss도 requires_gard = True인 tensor이 될 것이다.\n",
    "=> 따라서 여기다가 loss.backward()를 하면 computational graph형태로 되어있던 것이 계산이 된다 \n",
    "=> backward()는 변하는 것이 아니고 grad만 연산한느 것 (using torch.autograd)(실제 계산은 backward)\n",
    "\n",
    "1. **`torch.autograd` engine**\n",
    "    * use pytorch's built-in differentiation engine, `torch.autograd` => enables automatic computation of gradients for any computational graph (맨 밑에 이것에 대한 보충설명을 넣었다)\n",
    "1. **참고로, 밑에서 epoch=1을 해도 Accuracy가 96%로 나오는 것을 볼 수 있다**\n",
    "    * 이 이유는 **1 epoch $\\neq$ 1 update** (but rather, **1 epoch = (# total samples)/(batch size) updates**)\n",
    "1. `model.train()` VS `model.eval()` : https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch 여기보면 이해가 된다\n",
    "1. `optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)` 이라고 optimizer instance 정의 시, `model.parameters()`를 input으로 넣어줘야함! (왜냐하면 이 함수는 사실 어떠한 computational graph이든 되도록 만들어진 거라서, 이 경우 model.parameters()를 넣어줘야한다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating mode ON :\n",
    " * model.eval() \n",
    " * with torch.no_grad(): \n",
    "     dkslfjdlskfjdklsjfk\n",
    "\n",
    "## ASK\n",
    "\n",
    "* do I HAVE to use `model.train()` and `model.eval()` to set it to train/eval mode? because `https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html` 여기 pytorch documentation 에서는 그것들을 안해줘서..\n",
    "=> 보통은 주로 해준다 (.train()할떄 쓰는 process와 model.eval()을 할떄 쓰이는 process가 다르다 (ex : and so on)\n",
    "=> engine의 측면도있다 (dropout은 training할때는 넣고 forward에서는 죽이는데 그 eval을 안해주면 drouput이 활성화된 상태에서 inference를 해주는 것이라서)\n",
    "=> model.train()은 grad계산여부는 아니고 batch normalization등등을 정해주는 모드가 된다(모델안에서 train/eval할때 달라지는 것을 쓸때 이것을 실행시킨다)\n",
    "\n",
    "\n",
    "* why was `model.train()` turned on after `for epoch`?? what about before it? \n",
    "=> also possible, 조금 거슬리는 것이기는 하다(for loop안에 있으면)\n",
    "\n",
    "* 위에서 `Autograd` engine등등 적은 것이 맞나?\n",
    "* ask if my understanding of `loss` being just another tensor is correct\n",
    "\n",
    "====\n",
    "\n",
    "밑의 코드가 완전 좋은 것은 아니다 (epoch 을빼놓고 eㅔㅐ초\n",
    "=> 지금처럼 func안에 train loop을 돌리고 해도 되고\n",
    "=> train을 한 epoch만 돌리고 그것을 loop돌리는 것도 가능\n",
    "    => this case : better management (hieararchical)(hiearchy is important)(choosing this hiearchy is upto the programmer, but 최대한 깔끔하게 block 안/밖사이읭 영향이 있다거나 그러면 안좋다)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim.SGD?\n",
    "#help(nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Training Loop as a function\n",
    "#notice the arguments of the funciton makes this training loop very flexible (can be used on any model, loss function, etc)\n",
    "\n",
    "def train(model, optim, loss_fn, train_loader, epochs, device):\n",
    "    #let's run the training\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train() #TURN TRAINING MODE ON\n",
    "        for batch in train_loader:\n",
    "            input_tensor, target = batch #get data\n",
    "            input_tensor = input_tensor.to(device) #send to GPU \n",
    "            target = target.to(device)\n",
    "\n",
    "            #computing gradient using Autograd engine\n",
    "            optim.zero_grad()  #initialize\n",
    "            output = model(input_tensor) \n",
    "            loss = loss_fn(output, target) \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.data.item() #compute loss (i guess this is also an instance of using preexisting attribuet of loss?)\n",
    "        train_loss /= len(train_loader.dataset) #i.e. normalize training loss\n",
    "        print(f'Epoch: {epoch+1}, Training Loss: {train_loss}')\n",
    "        #print(\"===showing that the loss computed during training is just another Tensor, just with `requires_grad = True`===\\n\",\n",
    "              #type(loss), loss.requires_grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.021277688367664815\n"
     ]
    }
   ],
   "source": [
    "##실제로 optim, loss_fn 등의 것들을 정해서 training을 시키자!\n",
    "import torch.optim as optim\n",
    "\n",
    "#optimizer, loss_fn and other setting (arguments of the trainining loop function)\n",
    "optimizer = optim.SGD(network.parameters(), lr=1e-2, momentum=0.9) #i.e. parameters of custom NN as input, \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_loader = train_loader #이미 정의했었다\n",
    "num_epochs = 1\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "network.to(device)\n",
    "\n",
    "\n",
    "train(model = network, optim = optimizer, loss_fn = loss_function,\n",
    "     train_loader = train_loader, epochs = num_epochs, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note** : 위의 block을 여러번 돌리면  계속 training loss가 줄어든다 => 즉, weights가 (training loop돌릴때마다) optimize되는 것! (실제로 이렇게 for loop of trainining을 하도록 짤 수도 있다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필수\n",
    "**Requirements when making a testing loop**\n",
    "* `model.eval()` : turn **testing mode on**\n",
    "* `with torch.no_grad():` (**이게 없으면 memory 잡아먹음, turns off gradient tracking)\n",
    "* **calculate testing loss** by following steps: \n",
    "    * `output = model(input_tensor)` \n",
    "    * `loss = loss_fn(output, target)`\n",
    "* add-up losses and normalize (to get average testing loss)\n",
    "* `input_tensor.to(device)`, ... : tensor등을 device로 보내기!\n",
    "* FINAL : perform testing by calling `test(Xxkjfkldjf)` ...\n",
    "\n",
    "\n",
    "<br>\n",
    "common `test` loop function arguments \n",
    "* what model to test (`model` in code below) (required to compute the loss)\n",
    "* loss function definition (`loss_fn` in code below)\n",
    "* CPU or GPU (`device` in code below)\n",
    "\n",
    "\n",
    "<br>\n",
    "##\n",
    "* `https://pytorch.org/docs/stable/generated/torch.no_grad.html` : 여기서 `torch.no_grad` documentation보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASK\n",
    "* `with ` statement가 python 에서 무슨 의미?\n",
    "=> with 를 쓰는 것은 with밑의 있는 것들이 이 효과를 다 받도록 하는 것이다 (with하고 효과가 들어가는 상태에서 밑의 것을 하겠다)\n",
    "=> with syntax : 깔금하게 하기 위한것도 있다 \n",
    "=> with안넣고 torch.no_grad()만 하면 계속 no_grad가 되어서 계산이 안되는 것이 일언라수도 \n",
    "\n",
    "* `#test_loss += loss.data.item()` 이렇게 한 이유?? (그냥 `test_loss` 해도 되던데..?) #원래버젼 ##ASK \n",
    "=> loss를 마지막에 print/저장해야하는데 그러려면 torch가 아니라 integer/float형태여야한다\n",
    "=> 그래서, gpu위에있던 tensor을 cpu로 보내고 이것을 값만 뽑아내서 하고싶어서 저렇게 한다 \n",
    "=> `loss.detach().cpu().numpy` 이런식으로 cpu의 numpy로 하기도 한다 (마지막에 하는 것)\n",
    "=> 밑의 `.item()`은 scalar tensor일떄만 가능하다 \n",
    "=> 만약 backward안한 상태에서 item으로 뽑은 후 다시 backward를 하면 안된다 => 그래서 print를 하고싶다 이럴땜나 .tiem을 쓴다 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, test_loader,device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_tensor, target = batch\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            #calculate loss\n",
    "            output = model(input_tensor)\n",
    "            loss = loss_fn(output, target)\n",
    "            test_loss += loss#loss.data.item() #updated version, still works\n",
    "            #test_loss += loss.data.item() #원래버젼 ##ASK \n",
    "\n",
    "            pred = output.data.max(1, keepdim=True)[1] #이건 뭐지? 아마도 counting the number of correct things\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        print(\"loss : \", loss) ##added, remove later \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss},\\t Accuracy: ({100. * correct / len(test_loader.dataset)}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunmi/opt/anaconda3/envs/ML_tutor/lib/python3.8/site-packages/torch/nn/functional.py:1959: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(0.1856, device='mps:0')\n",
      "Test Loss: 0.0067834691144526005,\t Accuracy: (87.89000701904297%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#acutall calling and using the test loop\n",
    "test_loader = test_loader #test data\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") \n",
    "\n",
    "test(network, loss_fn = loss_function, test_loader = test_loader, device = device) #device also has to be specified!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Saving/Loading Model (덜완성)\n",
    "\n",
    "1. Option 1 : store parameters & structure of the model\n",
    "    * 그냥 완전한 복사라서 조금만 틀리면 날라감\n",
    "    * `torch.save(network, \"path/to/save/file\")`\n",
    "1. Option 2 : Weight값들을 dictionary 로 저장 \n",
    "    * recommended (b/c 이후 모델이 변겨되어도 사용가능) \n",
    "    * save **`network.state_dict()`** using `torch.save(network.satte_dict(), \"path/to/save/file\")\n",
    "\n",
    "근데 이 둘의 차이를 잘 모르겠다\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html\n",
    "\n",
    "https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
    "\n",
    "위에서 보듯이, optoin1은 거의 안쓰이고 무조건 state_dict로 save 하는듯...? 왜이러지\n",
    "(일단 두개 모두  \n",
    "\n",
    "\n",
    "**참고로, 이미 training 이 된 모델을 open source로 제공하는 경우가 있고, 이때 loading을 할 수 도있다**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=Oxt7KrqRPtQ&list=PL0E_1UqNACXDgPRWUBgShb6LMNP4wdqV1&index=4\n",
    "    \n",
    "여기보고 나머지 하기!\n",
    "\n",
    "\n",
    "**remember, `network` was the model which we trained`(object)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_NN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4_final): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "====looking at the parameters====\n",
      "<bound method Module.named_parameters of custom_NN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4_final): Linear(in_features=64, out_features=10, bias=True)\n",
      ")>\n",
      "name :  fc1.weight\n",
      "weights.shape :  torch.Size([128, 784])\n",
      "weights : \n",
      " Parameter containing:\n",
      "tensor([[-0.0334, -0.0074,  0.0343,  ...,  0.0030,  0.0175,  0.0156],\n",
      "        [-0.0196, -0.0277,  0.0099,  ..., -0.0171, -0.0012,  0.0054],\n",
      "        [-0.0259, -0.0299,  0.0079,  ...,  0.0295,  0.0313, -0.0205],\n",
      "        ...,\n",
      "        [ 0.0280, -0.0220, -0.0213,  ..., -0.0275, -0.0230,  0.0038],\n",
      "        [ 0.0087, -0.0176, -0.0105,  ..., -0.0162,  0.0119, -0.0118],\n",
      "        [-0.0093, -0.0012, -0.0111,  ...,  0.0266,  0.0206, -0.0031]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "name :  fc1.bias\n",
      "weights.shape :  torch.Size([128])\n",
      "weights : \n",
      " Parameter containing:\n",
      "tensor([ 0.0035,  0.0378,  0.0264,  0.0468, -0.0172,  0.0407,  0.0151, -0.0472,\n",
      "        -0.0084,  0.0258, -0.0369, -0.0010,  0.0127,  0.0006,  0.0596,  0.0405,\n",
      "         0.0503,  0.0048,  0.0538,  0.0516,  0.0316, -0.0159,  0.0330, -0.0844,\n",
      "         0.0188,  0.0634, -0.0164,  0.0117,  0.0444, -0.0145,  0.0056,  0.0185,\n",
      "        -0.0297,  0.0386,  0.0818,  0.0072,  0.0024,  0.0160,  0.0754, -0.0285,\n",
      "        -0.0045,  0.0453, -0.0112, -0.0191, -0.0086,  0.0239,  0.1653, -0.0144,\n",
      "        -0.0139,  0.0093,  0.0516,  0.0207,  0.0317,  0.0156, -0.0043,  0.0894,\n",
      "        -0.0220,  0.0385, -0.0393, -0.0542, -0.0133,  0.0030,  0.0022,  0.0067,\n",
      "         0.0158,  0.0497,  0.1028,  0.0818,  0.0418, -0.0339,  0.0406,  0.0605,\n",
      "         0.0436,  0.0093,  0.0141, -0.0146,  0.0375,  0.0041, -0.0073,  0.0035,\n",
      "         0.0642,  0.0297,  0.0174, -0.0147, -0.0188,  0.0192,  0.0109,  0.0068,\n",
      "         0.0141,  0.0131, -0.0095,  0.0056,  0.0326,  0.0395, -0.0049,  0.0473,\n",
      "         0.0586, -0.0236, -0.0196, -0.0232, -0.0104,  0.0249, -0.0070,  0.0089,\n",
      "        -0.0041,  0.0240,  0.0108,  0.0906, -0.0306,  0.0447,  0.0032,  0.0698,\n",
      "         0.0515, -0.0286,  0.0067, -0.0103, -0.0325, -0.0235, -0.0018,  0.0424,\n",
      "         0.0199,  0.0376,  0.0132,  0.0679, -0.0017, -0.0230,  0.0259,  0.0614],\n",
      "       device='mps:0', requires_grad=True)\n",
      "name :  fc2.weight\n",
      "weights.shape :  torch.Size([128, 128])\n",
      "weights : \n",
      " Parameter containing:\n",
      "tensor([[ 0.0529, -0.0775, -0.0731,  ...,  0.0451, -0.0367,  0.0928],\n",
      "        [-0.0612, -0.0545,  0.0555,  ...,  0.0182, -0.0301, -0.0754],\n",
      "        [ 0.0357,  0.1300, -0.0641,  ...,  0.0041, -0.0410, -0.1164],\n",
      "        ...,\n",
      "        [ 0.0714, -0.0294,  0.0576,  ..., -0.0278, -0.0979, -0.0565],\n",
      "        [-0.0156,  0.0172,  0.0976,  ..., -0.0701, -0.0740,  0.0317],\n",
      "        [-0.0197, -0.0324, -0.0133,  ..., -0.0166,  0.1230, -0.0680]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "name :  fc2.bias\n",
      "weights.shape :  torch.Size([128])\n",
      "weights : \n",
      " Parameter containing:\n",
      "tensor([ 0.0579,  0.0103,  0.1018,  0.0258,  0.0385, -0.0478, -0.0087,  0.0064,\n",
      "        -0.0892,  0.0478,  0.1014, -0.0803,  0.0409,  0.0813,  0.0677, -0.0210,\n",
      "        -0.0415,  0.0443,  0.0732,  0.0816,  0.1001, -0.0653, -0.0585,  0.0555,\n",
      "         0.1140,  0.0404,  0.0192,  0.0495,  0.0445, -0.0376,  0.0968,  0.0030,\n",
      "         0.0275,  0.0651,  0.0678, -0.0727, -0.0602, -0.0717,  0.0668, -0.0221,\n",
      "         0.0725,  0.0392,  0.0452, -0.0365, -0.0203,  0.0143, -0.0160,  0.0104,\n",
      "        -0.0096,  0.1221,  0.0031,  0.0702,  0.0246,  0.0081,  0.0313,  0.0423,\n",
      "         0.0816,  0.0151,  0.0622,  0.0607,  0.1016, -0.0391, -0.0312,  0.0710,\n",
      "         0.0702,  0.0573,  0.0205, -0.0534,  0.0616, -0.0844, -0.0613,  0.0026,\n",
      "        -0.0238,  0.0599,  0.1187,  0.0798,  0.0354,  0.0905,  0.0394, -0.0083,\n",
      "         0.0257,  0.1026, -0.0163,  0.0719,  0.0838, -0.0442, -0.0560,  0.0263,\n",
      "         0.0032,  0.0072,  0.0579, -0.0717,  0.0269, -0.0144,  0.0449,  0.0966,\n",
      "         0.0826,  0.0730,  0.0877, -0.0558,  0.0995,  0.0525, -0.0112,  0.0171,\n",
      "        -0.0872, -0.0715, -0.0053, -0.0837, -0.0583,  0.0165, -0.0140,  0.1180,\n",
      "        -0.0471, -0.0033, -0.0822,  0.0809,  0.0253,  0.0179,  0.0807,  0.1021,\n",
      "        -0.0582,  0.0970, -0.0330,  0.0716, -0.0693,  0.0141, -0.0291,  0.0043],\n",
      "       device='mps:0', requires_grad=True)\n",
      "name :  fc3.weight\n",
      "weights.shape :  torch.Size([64, 128])\n",
      "weights : \n",
      " Parameter containing:\n",
      "tensor([[ 0.0942, -0.0178,  0.0745,  ..., -0.0637, -0.0896,  0.0222],\n",
      "        [ 0.1573, -0.0584, -0.1352,  ..., -0.0055, -0.0170,  0.1412],\n",
      "        [-0.0386, -0.0081, -0.0303,  ...,  0.0196,  0.0018, -0.0042],\n",
      "        ...,\n",
      "        [-0.0607, -0.0307,  0.0875,  ...,  0.0172,  0.0308, -0.0368],\n",
      "        [-0.0635, -0.0423, -0.0114,  ...,  0.0178,  0.0436,  0.0154],\n",
      "        [-0.0321, -0.0316, -0.0044,  ..., -0.0198,  0.0400,  0.0500]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "name :  fc3.bias\n",
      "weights.shape :  torch.Size([64])\n",
      "weights : \n",
      " Parameter containing:\n",
      "tensor([-0.0557, -0.0344,  0.0119,  0.0204, -0.0155, -0.0655, -0.0455, -0.0766,\n",
      "         0.1097, -0.0096, -0.0573,  0.0507, -0.0724,  0.0080,  0.0606, -0.1083,\n",
      "        -0.1125, -0.1020, -0.0002, -0.1106, -0.0569, -0.1179, -0.0559, -0.0029,\n",
      "         0.0371,  0.0222, -0.0190,  0.1128, -0.0301, -0.0438,  0.0920, -0.0472,\n",
      "         0.0057, -0.0640,  0.0573, -0.0131,  0.0588, -0.0656,  0.0368, -0.0505,\n",
      "         0.0375, -0.0302,  0.0332, -0.0152,  0.0182,  0.0567, -0.0133,  0.0373,\n",
      "        -0.0641,  0.0038, -0.0047,  0.0958,  0.0139, -0.0280, -0.0174, -0.0145,\n",
      "        -0.0379,  0.0435, -0.0104,  0.0223,  0.0104,  0.0714, -0.0539,  0.0864],\n",
      "       device='mps:0', requires_grad=True)\n",
      "name :  fc4_final.weight\n",
      "weights.shape :  torch.Size([10, 64])\n",
      "weights : \n",
      " Parameter containing:\n",
      "tensor([[ 4.4931e-01,  4.9725e-01,  5.8200e-02,  1.2495e-01, -1.2624e-01,\n",
      "          2.0056e-01, -2.9786e-01, -2.5734e-01, -1.1318e-01,  2.5211e-01,\n",
      "         -1.4348e-01, -3.2195e-01,  1.4501e-01,  3.4035e-01, -1.8404e-01,\n",
      "          3.0807e-02,  6.7926e-02,  2.8370e-01,  4.2159e-01,  4.0496e-01,\n",
      "          6.0908e-02,  4.9340e-01, -3.7829e-01,  9.0874e-02,  1.8133e-01,\n",
      "         -4.1231e-01, -4.9796e-02, -1.7999e-01,  2.1268e-01, -4.0792e-01,\n",
      "         -5.6724e-01, -2.6553e-01, -3.2458e-01,  4.6156e-01, -5.3156e-01,\n",
      "         -1.2713e-01, -6.7435e-02, -1.4509e-01, -2.5282e-01, -2.3352e-01,\n",
      "         -4.5057e-01,  5.6681e-01,  2.6599e-01,  7.0481e-03, -2.6286e-01,\n",
      "          5.4666e-01,  3.1612e-01, -1.4695e-01,  1.4516e-01, -4.1058e-01,\n",
      "         -5.7984e-01, -1.5087e-01,  7.1109e-03, -4.0154e-02, -1.1559e-01,\n",
      "         -3.6403e-01, -6.4067e-01, -2.0041e-01, -4.3246e-01, -2.4424e-02,\n",
      "          3.0725e-01, -1.2045e-01, -8.7963e-02, -2.8625e-01],\n",
      "        [-4.9186e-01, -6.3658e-01, -2.1515e-01,  1.7423e-01, -6.5822e-02,\n",
      "         -5.2442e-01, -1.1626e-01, -1.8849e-01,  3.8973e-01, -3.7920e-01,\n",
      "          3.7357e-01,  9.6434e-02, -3.8747e-01, -4.5781e-01,  5.6078e-01,\n",
      "         -2.6213e-01, -2.8893e-01, -4.1662e-01,  5.0619e-02, -1.7407e-01,\n",
      "         -2.5078e-01, -3.2921e-01,  2.9601e-01, -4.1605e-02, -3.5338e-02,\n",
      "          1.4404e-01,  1.6397e-02,  3.6778e-01, -2.2920e-01, -2.5253e-01,\n",
      "          1.0260e-01,  2.8835e-01,  2.9586e-01,  4.2176e-02, -4.3189e-02,\n",
      "          3.6874e-01, -3.2682e-01, -4.5976e-02,  1.6734e-01,  1.8621e-01,\n",
      "          1.6082e-01, -2.7598e-01, -3.4944e-01, -1.9648e-01,  5.1033e-01,\n",
      "         -9.0574e-02, -2.6981e-01,  2.7424e-01, -4.1708e-02,  1.0556e-01,\n",
      "          5.2551e-01,  6.1866e-01,  1.9630e-01, -1.6859e-02,  3.1288e-01,\n",
      "         -2.3652e-01, -6.0573e-03,  3.5447e-01,  5.7346e-01,  3.5162e-01,\n",
      "         -3.0532e-01, -1.5800e-01,  2.2104e-01,  4.5570e-01],\n",
      "        [ 1.8666e-01,  2.9012e-01,  1.5949e-02,  2.8555e-01,  9.2121e-02,\n",
      "          4.9477e-02, -3.9719e-01,  2.0691e-01, -2.2593e-01, -8.8847e-02,\n",
      "          1.6999e-01, -1.4753e-01, -2.3092e-01,  2.7028e-01,  2.0720e-01,\n",
      "          3.3915e-01,  4.6298e-01, -6.6525e-02, -3.5388e-01,  4.3721e-01,\n",
      "          7.2603e-02, -3.1336e-01,  2.7212e-01,  3.6995e-02, -2.8018e-01,\n",
      "         -8.6218e-02, -2.0806e-01, -2.7342e-01, -2.4800e-01,  5.1805e-02,\n",
      "          7.8587e-02, -2.8543e-01, -2.9846e-01, -2.2006e-01,  2.8652e-01,\n",
      "         -3.0135e-01,  7.3288e-02, -2.1457e-01,  2.9707e-01, -2.9931e-02,\n",
      "          1.5531e-01, -1.4731e-01, -3.6599e-01, -3.4107e-01,  4.2168e-01,\n",
      "          2.1512e-01,  1.5148e-01,  3.8576e-01, -2.6533e-01,  2.1614e-01,\n",
      "          3.6404e-01, -1.4338e-01, -1.7293e-01, -4.0308e-02,  4.1622e-01,\n",
      "         -2.8108e-01,  2.1002e-02,  3.0431e-01,  6.5388e-02,  3.8284e-01,\n",
      "         -4.3699e-01, -3.0428e-01, -1.3798e-01, -6.1949e-02],\n",
      "        [-3.8126e-01,  4.6993e-01,  1.8298e-01,  2.5582e-01, -5.0543e-02,\n",
      "         -1.5384e-01, -2.5340e-01, -1.0109e-01, -2.3485e-02,  1.7154e-01,\n",
      "         -5.7843e-02,  4.1830e-01, -5.6733e-02, -3.4933e-02,  1.7653e-01,\n",
      "         -2.1673e-01, -2.6212e-01, -1.8931e-01, -4.9715e-03,  2.9787e-01,\n",
      "          1.8205e-01, -1.2477e-01, -3.0743e-01,  1.4407e-02, -1.8604e-01,\n",
      "         -2.5210e-01, -2.2118e-01, -2.7911e-01,  3.7750e-02, -1.1501e-01,\n",
      "         -4.4693e-01,  9.6478e-02, -2.4560e-01, -1.2448e-01,  4.1163e-01,\n",
      "          4.7459e-01, -3.1196e-01, -2.0108e-01, -5.4557e-02, -2.7854e-01,\n",
      "          3.4720e-01, -2.3488e-01,  1.0299e-01,  4.2076e-01,  1.2993e-01,\n",
      "          7.2770e-02,  2.1509e-01, -8.6683e-02, -6.6009e-02,  5.2351e-01,\n",
      "          4.5150e-01, -2.3074e-01, -1.3197e-02, -1.0956e-01,  5.1432e-01,\n",
      "         -2.6525e-02,  2.0422e-01,  4.7553e-03, -4.7354e-01,  5.1557e-02,\n",
      "         -4.2327e-02, -2.2275e-01,  4.5205e-01,  5.6733e-01],\n",
      "        [ 1.9454e-01, -5.8417e-01, -9.9509e-02, -4.5102e-01,  1.9133e-01,\n",
      "          1.8707e-01,  2.4762e-01,  6.4655e-01, -3.6597e-02, -1.0754e-01,\n",
      "         -3.1123e-01, -2.1767e-01, -1.2381e-01,  1.3002e-01, -2.3739e-01,\n",
      "         -7.8232e-02,  2.5773e-01,  2.0059e-01, -3.8130e-01, -2.9123e-01,\n",
      "         -3.2030e-01, -6.9405e-02,  1.8986e-01,  2.6015e-01,  2.5510e-01,\n",
      "          2.4115e-01,  5.0974e-01,  3.1193e-01, -1.4413e-01,  2.7226e-01,\n",
      "          2.3365e-01, -3.4124e-01,  4.3204e-01, -1.0045e-01, -7.3963e-02,\n",
      "         -5.6966e-01,  3.2337e-01,  5.0709e-01, -2.3035e-01,  4.5008e-01,\n",
      "         -1.1955e-01,  2.5986e-01, -3.5525e-03, -3.7268e-02, -2.7803e-01,\n",
      "         -2.7600e-01, -3.7124e-01,  4.0066e-02,  1.0005e-01, -1.5185e-01,\n",
      "         -3.4307e-01,  1.6438e-01,  1.1089e-01, -1.3569e-01, -1.0316e-01,\n",
      "          5.1990e-01,  1.8937e-01, -2.8976e-01,  8.2930e-02, -2.6150e-01,\n",
      "          2.2089e-01,  1.0637e-01, -1.1684e-01, -2.9136e-01],\n",
      "        [ 1.7368e-01,  3.4191e-01,  1.4998e-01,  7.8561e-02, -1.2505e-01,\n",
      "         -1.5714e-01,  5.1009e-02, -5.1918e-02, -2.0487e-01,  2.8607e-01,\n",
      "         -2.9523e-01, -1.2949e-01,  1.2062e-01,  3.2342e-01, -2.6885e-01,\n",
      "         -2.5147e-02, -1.4009e-01, -5.8089e-03,  4.6299e-01, -7.2358e-02,\n",
      "         -1.0023e-01,  3.1714e-01, -2.1379e-01,  2.5172e-01,  2.6550e-01,\n",
      "          3.3866e-04,  2.4808e-02, -3.1826e-01, -5.9879e-02, -6.8428e-02,\n",
      "         -1.4471e-01, -4.5893e-02, -8.8046e-02,  4.7996e-01,  1.4360e-01,\n",
      "          6.5942e-02, -4.7767e-02, -1.0981e-01,  5.6491e-02, -2.4074e-01,\n",
      "          4.1707e-01, -9.0505e-02,  2.5451e-01,  5.4281e-01, -3.2257e-03,\n",
      "         -9.0587e-02,  3.3902e-02, -8.7595e-02,  6.0177e-02, -8.3753e-03,\n",
      "          4.7934e-02,  6.2842e-02, -6.6387e-02, -8.3075e-02,  6.8677e-02,\n",
      "          2.9007e-01,  6.8172e-02, -2.8629e-01, -3.3903e-01,  1.2127e-01,\n",
      "          1.3386e-01,  2.7834e-01,  2.3395e-01,  5.7936e-02],\n",
      "        [ 2.2614e-01, -2.3366e-01, -1.1597e-01, -4.8197e-01, -2.3323e-01,\n",
      "         -7.5731e-02,  2.0895e-01,  5.5514e-01, -4.0692e-02, -2.6504e-01,\n",
      "          5.9411e-02, -2.9322e-01, -3.0311e-01,  2.8332e-01, -1.7601e-01,\n",
      "          1.7565e-01,  4.5510e-01, -2.0701e-01, -2.4861e-01,  1.8440e-01,\n",
      "         -2.9254e-01,  4.1617e-01,  3.3190e-01,  2.5334e-01, -1.5916e-03,\n",
      "          1.4374e-03,  5.0745e-01,  1.4613e-01, -4.9274e-01, -8.0711e-02,\n",
      "          2.7034e-01, -2.4156e-01,  1.0546e-01, -6.6042e-02, -1.2709e-01,\n",
      "         -5.1364e-01,  2.6114e-01, -3.1547e-01,  5.1855e-01, -2.0834e-01,\n",
      "         -2.8331e-01,  3.7053e-01,  3.9286e-02, -8.7203e-02,  4.3411e-01,\n",
      "          2.8193e-02,  1.9821e-01,  1.7799e-01,  2.2515e-01, -1.3976e-01,\n",
      "         -3.2453e-01,  7.6473e-03,  1.5161e-02, -2.1719e-01, -1.5473e-01,\n",
      "         -4.5320e-01, -4.4250e-01,  7.5509e-03,  2.7811e-01,  4.7189e-01,\n",
      "         -5.2561e-01,  9.0832e-03,  2.4208e-03, -3.7198e-01],\n",
      "        [-3.8592e-01,  2.3886e-01, -1.2005e-01, -1.9019e-01,  4.9837e-01,\n",
      "          2.1824e-01, -1.9373e-01, -4.9676e-01,  5.6625e-01,  5.2374e-02,\n",
      "          4.8669e-01,  1.7414e-01, -4.2070e-02, -5.3747e-01,  6.7163e-02,\n",
      "         -5.4389e-01, -3.3866e-01,  3.7859e-01,  3.2626e-01, -1.2191e-01,\n",
      "          4.0524e-01, -8.2852e-02, -4.0125e-01, -6.9828e-01, -2.0779e-02,\n",
      "         -3.7502e-01, -1.3705e-01,  2.3304e-01,  1.7769e-01,  4.5992e-01,\n",
      "          2.6338e-01,  4.4561e-01,  2.9915e-01, -2.4268e-03, -1.0597e-01,\n",
      "          2.5101e-01,  3.6103e-02,  3.8871e-01, -1.8881e-01,  2.9303e-01,\n",
      "         -3.4063e-01,  1.3654e-01, -6.6926e-02, -1.9336e-01, -2.4401e-01,\n",
      "         -2.0884e-01, -3.1630e-01,  1.3868e-01, -3.8061e-02, -3.2320e-02,\n",
      "         -1.2660e-01,  1.0512e-01,  6.4618e-01,  5.8540e-01, -1.3117e-01,\n",
      "          2.1117e-01,  6.5899e-02, -1.6767e-01, -2.4013e-01, -1.7885e-01,\n",
      "          1.6238e-01, -1.2672e-02,  5.2856e-02, -2.0095e-01],\n",
      "        [ 1.0118e-01,  2.7589e-01, -1.7108e-01,  3.3101e-01, -1.4629e-01,\n",
      "          6.3574e-02, -9.9080e-02, -1.8189e-01, -2.8798e-01,  1.2612e-01,\n",
      "         -2.9379e-01, -1.7705e-01,  3.2473e-01,  9.3471e-02, -4.7915e-02,\n",
      "          2.8992e-01, -3.4631e-02, -3.5465e-01, -3.7680e-01, -2.5062e-01,\n",
      "          1.0956e-01, -9.5622e-02,  3.0253e-01,  2.9540e-01, -2.2305e-01,\n",
      "          3.1857e-01, -3.0579e-01, -2.6797e-01,  2.3115e-01, -1.0792e-01,\n",
      "          3.6635e-01,  1.1986e-01, -2.8762e-01, -2.1548e-01,  3.9391e-01,\n",
      "          3.2493e-01,  1.3055e-01, -2.8456e-01, -1.4237e-01, -8.4788e-03,\n",
      "          2.9232e-01, -2.6221e-01, -7.8233e-02, -1.3340e-01, -3.0069e-01,\n",
      "         -4.5016e-01,  2.9576e-02, -1.0910e-01, -2.9123e-01, -1.6158e-01,\n",
      "          3.9487e-01, -2.6579e-01, -2.7292e-01, -2.5401e-01, -2.7871e-01,\n",
      "         -2.8551e-01,  4.2585e-01,  2.3901e-01,  1.5021e-01, -2.1821e-01,\n",
      "         -9.7370e-03,  1.3302e-01, -1.5064e-01, -6.1629e-02],\n",
      "        [ 8.8463e-02, -4.1013e-01, -1.1042e-01, -2.7800e-01,  4.2999e-02,\n",
      "          1.1725e-01,  2.1694e-01, -2.9312e-01,  9.6615e-02,  1.5526e-01,\n",
      "         -4.8710e-04,  2.4383e-01,  1.0646e-01, -2.5118e-01, -2.9607e-01,\n",
      "         -1.1902e-01, -1.1700e-01,  4.6466e-01, -1.2703e-01, -1.6903e-01,\n",
      "         -3.8784e-02, -1.0864e-01, -1.6279e-01, -2.7104e-01, -6.2019e-02,\n",
      "          3.1565e-01,  1.1144e-01,  3.3534e-01,  2.3361e-01,  3.4825e-01,\n",
      "          2.4860e-01,  4.4692e-01,  4.0520e-01, -2.8365e-01, -6.4166e-02,\n",
      "         -5.8881e-02,  4.8200e-02,  4.2778e-01, -2.0481e-01,  3.8120e-01,\n",
      "         -1.5886e-01,  5.9698e-02,  2.1616e-01, -7.9840e-02, -2.8346e-01,\n",
      "         -2.6784e-01, -2.4784e-01, -2.4056e-01, -1.3751e-01, -1.5365e-01,\n",
      "         -2.1583e-01, -1.4570e-01,  8.9237e-02,  3.5128e-01, -2.9957e-01,\n",
      "          4.8074e-01,  2.0596e-01, -2.0171e-01,  8.8487e-02, -2.7387e-01,\n",
      "          2.1261e-01,  1.6207e-01, -2.2365e-01, -2.5730e-01]], device='mps:0',\n",
      "       requires_grad=True)\n",
      "name :  fc4_final.bias\n",
      "weights.shape :  torch.Size([10])\n",
      "weights : \n",
      " Parameter containing:\n",
      "tensor([-0.0882,  0.0407,  0.0266, -0.0410, -0.1297,  0.0004,  0.0729, -0.0262,\n",
      "        -0.0518,  0.0341], device='mps:0', requires_grad=True)\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "## first, let's look at the model we're trying to save (the instance of the model)\n",
    "print(network)\n",
    "\n",
    "####looking at the parameters####\n",
    "print(\"\\n====looking at the parameters====\")\n",
    "print(network.named_parameters)\n",
    "#print(network.parameters) #same thing\n",
    "\n",
    "#finding the actual values of the parameters and stuff \n",
    "\n",
    "for i in network.named_parameters():\n",
    "    name, weights = i #i : tuple\n",
    "    print(\"name : \", name)\n",
    "    print(\"weights.shape : \", weights.shape)\n",
    "    print(\"weights : \\n\",weights) #weights : <class 'torch.nn.parameter.Parameter'>\n",
    "print(\"======\")\n",
    "\n",
    "\n",
    "###below : also works, but doens't give names\n",
    "# for i in network.parameters():\n",
    "#     print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/eunmi/Library/CloudStorage/OneDrive-성균관대학교/Cha's Lab/STUDY(prob,stat,GCN,Pytorch)/ML_tutoring/CODES_and_STUFF\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_NN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4_final): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "<class '__main__.custom_NN'>\n"
     ]
    }
   ],
   "source": [
    "## option 1 : save EVERYTHING (i.e. copy)\n",
    "\n",
    "##saving\n",
    "torch.save(network, \"./lab1_saves/save_option1\" ) #\n",
    "\n",
    "##loading\n",
    "loaded_network = torch.load(\"./lab1_saves/save_option1\")\n",
    "\n",
    "print(loaded_network) #looking at the loaded network, it's the same\n",
    "print(type(loaded_network)) #i.e. a class! (which is expected)\n",
    "\n",
    "#실제로 individual parameters print해도 똑같다는 것을 볼 수 있다 (생략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.0334, -0.0074,  0.0343,  ...,  0.0030,  0.0175,  0.0156],\n",
      "        [-0.0196, -0.0277,  0.0099,  ..., -0.0171, -0.0012,  0.0054],\n",
      "        [-0.0259, -0.0299,  0.0079,  ...,  0.0295,  0.0313, -0.0205],\n",
      "        ...,\n",
      "        [ 0.0280, -0.0220, -0.0213,  ..., -0.0275, -0.0230,  0.0038],\n",
      "        [ 0.0087, -0.0176, -0.0105,  ..., -0.0162,  0.0119, -0.0118],\n",
      "        [-0.0093, -0.0012, -0.0111,  ...,  0.0266,  0.0206, -0.0031]],\n",
      "       device='mps:0')), ('fc1.bias', tensor([ 0.0035,  0.0378,  0.0264,  0.0468, -0.0172,  0.0407,  0.0151, -0.0472,\n",
      "        -0.0084,  0.0258, -0.0369, -0.0010,  0.0127,  0.0006,  0.0596,  0.0405,\n",
      "         0.0503,  0.0048,  0.0538,  0.0516,  0.0316, -0.0159,  0.0330, -0.0844,\n",
      "         0.0188,  0.0634, -0.0164,  0.0117,  0.0444, -0.0145,  0.0056,  0.0185,\n",
      "        -0.0297,  0.0386,  0.0818,  0.0072,  0.0024,  0.0160,  0.0754, -0.0285,\n",
      "        -0.0045,  0.0453, -0.0112, -0.0191, -0.0086,  0.0239,  0.1653, -0.0144,\n",
      "        -0.0139,  0.0093,  0.0516,  0.0207,  0.0317,  0.0156, -0.0043,  0.0894,\n",
      "        -0.0220,  0.0385, -0.0393, -0.0542, -0.0133,  0.0030,  0.0022,  0.0067,\n",
      "         0.0158,  0.0497,  0.1028,  0.0818,  0.0418, -0.0339,  0.0406,  0.0605,\n",
      "         0.0436,  0.0093,  0.0141, -0.0146,  0.0375,  0.0041, -0.0073,  0.0035,\n",
      "         0.0642,  0.0297,  0.0174, -0.0147, -0.0188,  0.0192,  0.0109,  0.0068,\n",
      "         0.0141,  0.0131, -0.0095,  0.0056,  0.0326,  0.0395, -0.0049,  0.0473,\n",
      "         0.0586, -0.0236, -0.0196, -0.0232, -0.0104,  0.0249, -0.0070,  0.0089,\n",
      "        -0.0041,  0.0240,  0.0108,  0.0906, -0.0306,  0.0447,  0.0032,  0.0698,\n",
      "         0.0515, -0.0286,  0.0067, -0.0103, -0.0325, -0.0235, -0.0018,  0.0424,\n",
      "         0.0199,  0.0376,  0.0132,  0.0679, -0.0017, -0.0230,  0.0259,  0.0614],\n",
      "       device='mps:0')), ('fc2.weight', tensor([[ 0.0529, -0.0775, -0.0731,  ...,  0.0451, -0.0367,  0.0928],\n",
      "        [-0.0612, -0.0545,  0.0555,  ...,  0.0182, -0.0301, -0.0754],\n",
      "        [ 0.0357,  0.1300, -0.0641,  ...,  0.0041, -0.0410, -0.1164],\n",
      "        ...,\n",
      "        [ 0.0714, -0.0294,  0.0576,  ..., -0.0278, -0.0979, -0.0565],\n",
      "        [-0.0156,  0.0172,  0.0976,  ..., -0.0701, -0.0740,  0.0317],\n",
      "        [-0.0197, -0.0324, -0.0133,  ..., -0.0166,  0.1230, -0.0680]],\n",
      "       device='mps:0')), ('fc2.bias', tensor([ 0.0579,  0.0103,  0.1018,  0.0258,  0.0385, -0.0478, -0.0087,  0.0064,\n",
      "        -0.0892,  0.0478,  0.1014, -0.0803,  0.0409,  0.0813,  0.0677, -0.0210,\n",
      "        -0.0415,  0.0443,  0.0732,  0.0816,  0.1001, -0.0653, -0.0585,  0.0555,\n",
      "         0.1140,  0.0404,  0.0192,  0.0495,  0.0445, -0.0376,  0.0968,  0.0030,\n",
      "         0.0275,  0.0651,  0.0678, -0.0727, -0.0602, -0.0717,  0.0668, -0.0221,\n",
      "         0.0725,  0.0392,  0.0452, -0.0365, -0.0203,  0.0143, -0.0160,  0.0104,\n",
      "        -0.0096,  0.1221,  0.0031,  0.0702,  0.0246,  0.0081,  0.0313,  0.0423,\n",
      "         0.0816,  0.0151,  0.0622,  0.0607,  0.1016, -0.0391, -0.0312,  0.0710,\n",
      "         0.0702,  0.0573,  0.0205, -0.0534,  0.0616, -0.0844, -0.0613,  0.0026,\n",
      "        -0.0238,  0.0599,  0.1187,  0.0798,  0.0354,  0.0905,  0.0394, -0.0083,\n",
      "         0.0257,  0.1026, -0.0163,  0.0719,  0.0838, -0.0442, -0.0560,  0.0263,\n",
      "         0.0032,  0.0072,  0.0579, -0.0717,  0.0269, -0.0144,  0.0449,  0.0966,\n",
      "         0.0826,  0.0730,  0.0877, -0.0558,  0.0995,  0.0525, -0.0112,  0.0171,\n",
      "        -0.0872, -0.0715, -0.0053, -0.0837, -0.0583,  0.0165, -0.0140,  0.1180,\n",
      "        -0.0471, -0.0033, -0.0822,  0.0809,  0.0253,  0.0179,  0.0807,  0.1021,\n",
      "        -0.0582,  0.0970, -0.0330,  0.0716, -0.0693,  0.0141, -0.0291,  0.0043],\n",
      "       device='mps:0')), ('fc3.weight', tensor([[ 0.0942, -0.0178,  0.0745,  ..., -0.0637, -0.0896,  0.0222],\n",
      "        [ 0.1573, -0.0584, -0.1352,  ..., -0.0055, -0.0170,  0.1412],\n",
      "        [-0.0386, -0.0081, -0.0303,  ...,  0.0196,  0.0018, -0.0042],\n",
      "        ...,\n",
      "        [-0.0607, -0.0307,  0.0875,  ...,  0.0172,  0.0308, -0.0368],\n",
      "        [-0.0635, -0.0423, -0.0114,  ...,  0.0178,  0.0436,  0.0154],\n",
      "        [-0.0321, -0.0316, -0.0044,  ..., -0.0198,  0.0400,  0.0500]],\n",
      "       device='mps:0')), ('fc3.bias', tensor([-0.0557, -0.0344,  0.0119,  0.0204, -0.0155, -0.0655, -0.0455, -0.0766,\n",
      "         0.1097, -0.0096, -0.0573,  0.0507, -0.0724,  0.0080,  0.0606, -0.1083,\n",
      "        -0.1125, -0.1020, -0.0002, -0.1106, -0.0569, -0.1179, -0.0559, -0.0029,\n",
      "         0.0371,  0.0222, -0.0190,  0.1128, -0.0301, -0.0438,  0.0920, -0.0472,\n",
      "         0.0057, -0.0640,  0.0573, -0.0131,  0.0588, -0.0656,  0.0368, -0.0505,\n",
      "         0.0375, -0.0302,  0.0332, -0.0152,  0.0182,  0.0567, -0.0133,  0.0373,\n",
      "        -0.0641,  0.0038, -0.0047,  0.0958,  0.0139, -0.0280, -0.0174, -0.0145,\n",
      "        -0.0379,  0.0435, -0.0104,  0.0223,  0.0104,  0.0714, -0.0539,  0.0864],\n",
      "       device='mps:0')), ('fc4_final.weight', tensor([[ 4.4931e-01,  4.9725e-01,  5.8200e-02,  1.2495e-01, -1.2624e-01,\n",
      "          2.0056e-01, -2.9786e-01, -2.5734e-01, -1.1318e-01,  2.5211e-01,\n",
      "         -1.4348e-01, -3.2195e-01,  1.4501e-01,  3.4035e-01, -1.8404e-01,\n",
      "          3.0807e-02,  6.7926e-02,  2.8370e-01,  4.2159e-01,  4.0496e-01,\n",
      "          6.0908e-02,  4.9340e-01, -3.7829e-01,  9.0874e-02,  1.8133e-01,\n",
      "         -4.1231e-01, -4.9796e-02, -1.7999e-01,  2.1268e-01, -4.0792e-01,\n",
      "         -5.6724e-01, -2.6553e-01, -3.2458e-01,  4.6156e-01, -5.3156e-01,\n",
      "         -1.2713e-01, -6.7435e-02, -1.4509e-01, -2.5282e-01, -2.3352e-01,\n",
      "         -4.5057e-01,  5.6681e-01,  2.6599e-01,  7.0481e-03, -2.6286e-01,\n",
      "          5.4666e-01,  3.1612e-01, -1.4695e-01,  1.4516e-01, -4.1058e-01,\n",
      "         -5.7984e-01, -1.5087e-01,  7.1109e-03, -4.0154e-02, -1.1559e-01,\n",
      "         -3.6403e-01, -6.4067e-01, -2.0041e-01, -4.3246e-01, -2.4424e-02,\n",
      "          3.0725e-01, -1.2045e-01, -8.7963e-02, -2.8625e-01],\n",
      "        [-4.9186e-01, -6.3658e-01, -2.1515e-01,  1.7423e-01, -6.5822e-02,\n",
      "         -5.2442e-01, -1.1626e-01, -1.8849e-01,  3.8973e-01, -3.7920e-01,\n",
      "          3.7357e-01,  9.6434e-02, -3.8747e-01, -4.5781e-01,  5.6078e-01,\n",
      "         -2.6213e-01, -2.8893e-01, -4.1662e-01,  5.0619e-02, -1.7407e-01,\n",
      "         -2.5078e-01, -3.2921e-01,  2.9601e-01, -4.1605e-02, -3.5338e-02,\n",
      "          1.4404e-01,  1.6397e-02,  3.6778e-01, -2.2920e-01, -2.5253e-01,\n",
      "          1.0260e-01,  2.8835e-01,  2.9586e-01,  4.2176e-02, -4.3189e-02,\n",
      "          3.6874e-01, -3.2682e-01, -4.5976e-02,  1.6734e-01,  1.8621e-01,\n",
      "          1.6082e-01, -2.7598e-01, -3.4944e-01, -1.9648e-01,  5.1033e-01,\n",
      "         -9.0574e-02, -2.6981e-01,  2.7424e-01, -4.1708e-02,  1.0556e-01,\n",
      "          5.2551e-01,  6.1866e-01,  1.9630e-01, -1.6859e-02,  3.1288e-01,\n",
      "         -2.3652e-01, -6.0573e-03,  3.5447e-01,  5.7346e-01,  3.5162e-01,\n",
      "         -3.0532e-01, -1.5800e-01,  2.2104e-01,  4.5570e-01],\n",
      "        [ 1.8666e-01,  2.9012e-01,  1.5949e-02,  2.8555e-01,  9.2121e-02,\n",
      "          4.9477e-02, -3.9719e-01,  2.0691e-01, -2.2593e-01, -8.8847e-02,\n",
      "          1.6999e-01, -1.4753e-01, -2.3092e-01,  2.7028e-01,  2.0720e-01,\n",
      "          3.3915e-01,  4.6298e-01, -6.6525e-02, -3.5388e-01,  4.3721e-01,\n",
      "          7.2603e-02, -3.1336e-01,  2.7212e-01,  3.6995e-02, -2.8018e-01,\n",
      "         -8.6218e-02, -2.0806e-01, -2.7342e-01, -2.4800e-01,  5.1805e-02,\n",
      "          7.8587e-02, -2.8543e-01, -2.9846e-01, -2.2006e-01,  2.8652e-01,\n",
      "         -3.0135e-01,  7.3288e-02, -2.1457e-01,  2.9707e-01, -2.9931e-02,\n",
      "          1.5531e-01, -1.4731e-01, -3.6599e-01, -3.4107e-01,  4.2168e-01,\n",
      "          2.1512e-01,  1.5148e-01,  3.8576e-01, -2.6533e-01,  2.1614e-01,\n",
      "          3.6404e-01, -1.4338e-01, -1.7293e-01, -4.0308e-02,  4.1622e-01,\n",
      "         -2.8108e-01,  2.1002e-02,  3.0431e-01,  6.5388e-02,  3.8284e-01,\n",
      "         -4.3699e-01, -3.0428e-01, -1.3798e-01, -6.1949e-02],\n",
      "        [-3.8126e-01,  4.6993e-01,  1.8298e-01,  2.5582e-01, -5.0543e-02,\n",
      "         -1.5384e-01, -2.5340e-01, -1.0109e-01, -2.3485e-02,  1.7154e-01,\n",
      "         -5.7843e-02,  4.1830e-01, -5.6733e-02, -3.4933e-02,  1.7653e-01,\n",
      "         -2.1673e-01, -2.6212e-01, -1.8931e-01, -4.9715e-03,  2.9787e-01,\n",
      "          1.8205e-01, -1.2477e-01, -3.0743e-01,  1.4407e-02, -1.8604e-01,\n",
      "         -2.5210e-01, -2.2118e-01, -2.7911e-01,  3.7750e-02, -1.1501e-01,\n",
      "         -4.4693e-01,  9.6478e-02, -2.4560e-01, -1.2448e-01,  4.1163e-01,\n",
      "          4.7459e-01, -3.1196e-01, -2.0108e-01, -5.4557e-02, -2.7854e-01,\n",
      "          3.4720e-01, -2.3488e-01,  1.0299e-01,  4.2076e-01,  1.2993e-01,\n",
      "          7.2770e-02,  2.1509e-01, -8.6683e-02, -6.6009e-02,  5.2351e-01,\n",
      "          4.5150e-01, -2.3074e-01, -1.3197e-02, -1.0956e-01,  5.1432e-01,\n",
      "         -2.6525e-02,  2.0422e-01,  4.7553e-03, -4.7354e-01,  5.1557e-02,\n",
      "         -4.2327e-02, -2.2275e-01,  4.5205e-01,  5.6733e-01],\n",
      "        [ 1.9454e-01, -5.8417e-01, -9.9509e-02, -4.5102e-01,  1.9133e-01,\n",
      "          1.8707e-01,  2.4762e-01,  6.4655e-01, -3.6597e-02, -1.0754e-01,\n",
      "         -3.1123e-01, -2.1767e-01, -1.2381e-01,  1.3002e-01, -2.3739e-01,\n",
      "         -7.8232e-02,  2.5773e-01,  2.0059e-01, -3.8130e-01, -2.9123e-01,\n",
      "         -3.2030e-01, -6.9405e-02,  1.8986e-01,  2.6015e-01,  2.5510e-01,\n",
      "          2.4115e-01,  5.0974e-01,  3.1193e-01, -1.4413e-01,  2.7226e-01,\n",
      "          2.3365e-01, -3.4124e-01,  4.3204e-01, -1.0045e-01, -7.3963e-02,\n",
      "         -5.6966e-01,  3.2337e-01,  5.0709e-01, -2.3035e-01,  4.5008e-01,\n",
      "         -1.1955e-01,  2.5986e-01, -3.5525e-03, -3.7268e-02, -2.7803e-01,\n",
      "         -2.7600e-01, -3.7124e-01,  4.0066e-02,  1.0005e-01, -1.5185e-01,\n",
      "         -3.4307e-01,  1.6438e-01,  1.1089e-01, -1.3569e-01, -1.0316e-01,\n",
      "          5.1990e-01,  1.8937e-01, -2.8976e-01,  8.2930e-02, -2.6150e-01,\n",
      "          2.2089e-01,  1.0637e-01, -1.1684e-01, -2.9136e-01],\n",
      "        [ 1.7368e-01,  3.4191e-01,  1.4998e-01,  7.8561e-02, -1.2505e-01,\n",
      "         -1.5714e-01,  5.1009e-02, -5.1918e-02, -2.0487e-01,  2.8607e-01,\n",
      "         -2.9523e-01, -1.2949e-01,  1.2062e-01,  3.2342e-01, -2.6885e-01,\n",
      "         -2.5147e-02, -1.4009e-01, -5.8089e-03,  4.6299e-01, -7.2358e-02,\n",
      "         -1.0023e-01,  3.1714e-01, -2.1379e-01,  2.5172e-01,  2.6550e-01,\n",
      "          3.3866e-04,  2.4808e-02, -3.1826e-01, -5.9879e-02, -6.8428e-02,\n",
      "         -1.4471e-01, -4.5893e-02, -8.8046e-02,  4.7996e-01,  1.4360e-01,\n",
      "          6.5942e-02, -4.7767e-02, -1.0981e-01,  5.6491e-02, -2.4074e-01,\n",
      "          4.1707e-01, -9.0505e-02,  2.5451e-01,  5.4281e-01, -3.2257e-03,\n",
      "         -9.0587e-02,  3.3902e-02, -8.7595e-02,  6.0177e-02, -8.3753e-03,\n",
      "          4.7934e-02,  6.2842e-02, -6.6387e-02, -8.3075e-02,  6.8677e-02,\n",
      "          2.9007e-01,  6.8172e-02, -2.8629e-01, -3.3903e-01,  1.2127e-01,\n",
      "          1.3386e-01,  2.7834e-01,  2.3395e-01,  5.7936e-02],\n",
      "        [ 2.2614e-01, -2.3366e-01, -1.1597e-01, -4.8197e-01, -2.3323e-01,\n",
      "         -7.5731e-02,  2.0895e-01,  5.5514e-01, -4.0692e-02, -2.6504e-01,\n",
      "          5.9411e-02, -2.9322e-01, -3.0311e-01,  2.8332e-01, -1.7601e-01,\n",
      "          1.7565e-01,  4.5510e-01, -2.0701e-01, -2.4861e-01,  1.8440e-01,\n",
      "         -2.9254e-01,  4.1617e-01,  3.3190e-01,  2.5334e-01, -1.5916e-03,\n",
      "          1.4374e-03,  5.0745e-01,  1.4613e-01, -4.9274e-01, -8.0711e-02,\n",
      "          2.7034e-01, -2.4156e-01,  1.0546e-01, -6.6042e-02, -1.2709e-01,\n",
      "         -5.1364e-01,  2.6114e-01, -3.1547e-01,  5.1855e-01, -2.0834e-01,\n",
      "         -2.8331e-01,  3.7053e-01,  3.9286e-02, -8.7203e-02,  4.3411e-01,\n",
      "          2.8193e-02,  1.9821e-01,  1.7799e-01,  2.2515e-01, -1.3976e-01,\n",
      "         -3.2453e-01,  7.6473e-03,  1.5161e-02, -2.1719e-01, -1.5473e-01,\n",
      "         -4.5320e-01, -4.4250e-01,  7.5509e-03,  2.7811e-01,  4.7189e-01,\n",
      "         -5.2561e-01,  9.0832e-03,  2.4208e-03, -3.7198e-01],\n",
      "        [-3.8592e-01,  2.3886e-01, -1.2005e-01, -1.9019e-01,  4.9837e-01,\n",
      "          2.1824e-01, -1.9373e-01, -4.9676e-01,  5.6625e-01,  5.2374e-02,\n",
      "          4.8669e-01,  1.7414e-01, -4.2070e-02, -5.3747e-01,  6.7163e-02,\n",
      "         -5.4389e-01, -3.3866e-01,  3.7859e-01,  3.2626e-01, -1.2191e-01,\n",
      "          4.0524e-01, -8.2852e-02, -4.0125e-01, -6.9828e-01, -2.0779e-02,\n",
      "         -3.7502e-01, -1.3705e-01,  2.3304e-01,  1.7769e-01,  4.5992e-01,\n",
      "          2.6338e-01,  4.4561e-01,  2.9915e-01, -2.4268e-03, -1.0597e-01,\n",
      "          2.5101e-01,  3.6103e-02,  3.8871e-01, -1.8881e-01,  2.9303e-01,\n",
      "         -3.4063e-01,  1.3654e-01, -6.6926e-02, -1.9336e-01, -2.4401e-01,\n",
      "         -2.0884e-01, -3.1630e-01,  1.3868e-01, -3.8061e-02, -3.2320e-02,\n",
      "         -1.2660e-01,  1.0512e-01,  6.4618e-01,  5.8540e-01, -1.3117e-01,\n",
      "          2.1117e-01,  6.5899e-02, -1.6767e-01, -2.4013e-01, -1.7885e-01,\n",
      "          1.6238e-01, -1.2672e-02,  5.2856e-02, -2.0095e-01],\n",
      "        [ 1.0118e-01,  2.7589e-01, -1.7108e-01,  3.3101e-01, -1.4629e-01,\n",
      "          6.3574e-02, -9.9080e-02, -1.8189e-01, -2.8798e-01,  1.2612e-01,\n",
      "         -2.9379e-01, -1.7705e-01,  3.2473e-01,  9.3471e-02, -4.7915e-02,\n",
      "          2.8992e-01, -3.4631e-02, -3.5465e-01, -3.7680e-01, -2.5062e-01,\n",
      "          1.0956e-01, -9.5622e-02,  3.0253e-01,  2.9540e-01, -2.2305e-01,\n",
      "          3.1857e-01, -3.0579e-01, -2.6797e-01,  2.3115e-01, -1.0792e-01,\n",
      "          3.6635e-01,  1.1986e-01, -2.8762e-01, -2.1548e-01,  3.9391e-01,\n",
      "          3.2493e-01,  1.3055e-01, -2.8456e-01, -1.4237e-01, -8.4788e-03,\n",
      "          2.9232e-01, -2.6221e-01, -7.8233e-02, -1.3340e-01, -3.0069e-01,\n",
      "         -4.5016e-01,  2.9576e-02, -1.0910e-01, -2.9123e-01, -1.6158e-01,\n",
      "          3.9487e-01, -2.6579e-01, -2.7292e-01, -2.5401e-01, -2.7871e-01,\n",
      "         -2.8551e-01,  4.2585e-01,  2.3901e-01,  1.5021e-01, -2.1821e-01,\n",
      "         -9.7370e-03,  1.3302e-01, -1.5064e-01, -6.1629e-02],\n",
      "        [ 8.8463e-02, -4.1013e-01, -1.1042e-01, -2.7800e-01,  4.2999e-02,\n",
      "          1.1725e-01,  2.1694e-01, -2.9312e-01,  9.6615e-02,  1.5526e-01,\n",
      "         -4.8710e-04,  2.4383e-01,  1.0646e-01, -2.5118e-01, -2.9607e-01,\n",
      "         -1.1902e-01, -1.1700e-01,  4.6466e-01, -1.2703e-01, -1.6903e-01,\n",
      "         -3.8784e-02, -1.0864e-01, -1.6279e-01, -2.7104e-01, -6.2019e-02,\n",
      "          3.1565e-01,  1.1144e-01,  3.3534e-01,  2.3361e-01,  3.4825e-01,\n",
      "          2.4860e-01,  4.4692e-01,  4.0520e-01, -2.8365e-01, -6.4166e-02,\n",
      "         -5.8881e-02,  4.8200e-02,  4.2778e-01, -2.0481e-01,  3.8120e-01,\n",
      "         -1.5886e-01,  5.9698e-02,  2.1616e-01, -7.9840e-02, -2.8346e-01,\n",
      "         -2.6784e-01, -2.4784e-01, -2.4056e-01, -1.3751e-01, -1.5365e-01,\n",
      "         -2.1583e-01, -1.4570e-01,  8.9237e-02,  3.5128e-01, -2.9957e-01,\n",
      "          4.8074e-01,  2.0596e-01, -2.0171e-01,  8.8487e-02, -2.7387e-01,\n",
      "          2.1261e-01,  1.6207e-01, -2.2365e-01, -2.5730e-01]], device='mps:0')), ('fc4_final.bias', tensor([-0.0882,  0.0407,  0.0266, -0.0410, -0.1297,  0.0004,  0.0729, -0.0262,\n",
      "        -0.0518,  0.0341], device='mps:0'))])\n"
     ]
    }
   ],
   "source": [
    "## option 2 : save as DICTIONARY\n",
    "#save the maps of each layer's parameters as DICTIONARY\n",
    "\n",
    "print(network.state_dict())\n",
    "torch.save(network.state_dict(), \"./lab1_saves/save_option2\")\n",
    "#torch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Autogerad`에 대한  보충설명 (from pytorch website)\n",
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "(밑의 예시 보기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#즉, documentation에서 보듯이, tensor 안에 `requires_grad = True`로 된 것들에 대해 Autorgrad가 일어나게 된다!\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True) # adjustable tensor!\n",
    "b = torch.randn(3, requires_grad=True) #adjustable tensor!\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "print(z)\n",
    "print(loss)\n",
    "\n",
    "print(\"====before loss.backward()====\")\n",
    "print(\"w.grad : \",w.grad)\n",
    "print(\"b.grad : \",b.grad)\n",
    "print(\"x.grad : \",x.grad)\n",
    "print(\"y.grad : \",y.grad)\n",
    "print(\"z.grad : \",z.grad)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "print(\"====after loss.backward()====\")\n",
    "\n",
    "print(\"w.grad : \",w.grad)\n",
    "print(\"b.grad : \",b.grad)\n",
    "print(\"x.grad : \",x.grad)\n",
    "print(\"y.grad : \",y.grad)\n",
    "print(\"z.grad : \",z.grad)\n",
    "\n",
    "\n",
    "\n",
    "print(\"====disable gradient tracking====\")\n",
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collections : python내부 library, 이 안에 ordersdict라는 dictionary 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add module로 하나 추가 예시로 해주면, \n",
    "#더 추가된 것을 볼 수 있다\n",
    "\n",
    "#이런식으로 해서 다시 trianing을 할 수 있다!\n",
    "\n",
    "#strc\n",
    "\n",
    "#strict = True 에서 no key => becuase we added new thing\n",
    "#그래서 이럴때는 strict = False쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex : \n",
    "#ResNET가져오되, 1000 class에서 10으로 가도록 additional layer을 추가해서 train/test 가능하다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
