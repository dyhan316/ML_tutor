{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FKT9oKKgiuYJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "#import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NedzSvufilUn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.12.0, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))\n",
    "#print(\"Using torchtext version: {}\".format(torchtext.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ply4nFhlW2cB"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_94PQ3PxW38w"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scheduler = None):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    tqdm_bar = tqdm(enumerate(train_loader))\n",
    "    for batch_idx, (image, label) in tqdm_bar:\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        prediction = output.max(1, keepdim = True)[1]\n",
    "        correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "        optimizer.step()\n",
    "        tqdm_bar.set_description(\"Epoch {} - train loss: {:.6f}\".format(epoch, loss.item()))\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in tqdm(test_loader):\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6ETrmq4GWeOf"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(3 * 13, 4))\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.plot(history['train_loss'], label=\"train_loss\")\n",
    "    plt.plot(history['test_loss'], label=\"test_loss\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.plot(history['train_acc'], label=\"train_acc\")\n",
    "    plt.plot(history['test_acc'], label=\"test_acc\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.title(\"Learning Rate\")\n",
    "    plt.plot(history['lr'], label=\"learning rate\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"LR\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oVWSuO1yhPg6"
   },
   "outputs": [],
   "source": [
    "def plot_dataset(dataloader, grid_width=8, grid_height=2, figure_width=12, figure_height=3, y_hats=None):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    f, ax = plt.subplots(grid_height, grid_width)\n",
    "    f.set_size_inches(figure_width, figure_height)\n",
    "    img_idx = 0\n",
    "    for i in range(0, grid_height):\n",
    "        for j in range(0, grid_width):\n",
    "            image = images[img_idx]\n",
    "            label = labels[img_idx]\n",
    "            title_color = 'k'\n",
    "            if y_hats is None:\n",
    "                label_idx = int(label)\n",
    "            else:\n",
    "                label_idx = int(y_hats[img_idx])\n",
    "                if int(labels[img_idx]) != label_idx:\n",
    "                    title_color = 'r'\n",
    "            label = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'][label_idx]\n",
    "            ax[i][j].axis('off')\n",
    "            ax[i][j].set_title(label, color=title_color)\n",
    "            image = image.squeeze(axis=0)\n",
    "            ax[i][j].imshow(image, cmap=plt.get_cmap('gray'))\n",
    "            img_idx += 1\n",
    "        plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts25MjwhqyRd"
   },
   "source": [
    "# Load Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea7bwXqRd5Cg"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./data/', train=True, download=True,\n",
    "                          transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                        transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./data/', train=False, transform=transforms.Compose([\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.1307,), (0.3081,))\n",
    "                          ])),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ApCjv_0ghHdS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_dataset\u001b[49m(train_loader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "plot_dataset(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpZtPi6Rfdz8"
   },
   "source": [
    "# Custom Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKjmf4kNSWZ1"
   },
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                             \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):\n",
    "        #### TODO ####\n",
    "\n",
    "        ##############\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + str(self.p) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7djdSI4e-WG"
   },
   "outputs": [],
   "source": [
    "x = torch.rand((3,2))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DG0Is7Yfa7X"
   },
   "outputs": [],
   "source": [
    "Dropout()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F50ufKh2fV9g"
   },
   "outputs": [],
   "source": [
    "# What happens if dropout ratio is too low (= no dropout)?\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maUACQq6e8At"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "model = SimpleClassifier().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "history = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[], 'lr':[]}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tLR: {:.4f}, \\tTrain Loss: {:.4f}, \\tTrain Accuracy: {:.2f} %, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], train_loss, train_accuracy, test_loss, test_accuracy))\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_accuracy)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_accuracy)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAs7s_8Bd_jQ"
   },
   "outputs": [],
   "source": [
    "class RegClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        self.dropout = Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.fc4(x),dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eMr_s_oeOLO"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "model = RegClassifier().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "history = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[], 'lr':[]}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tLR: {:.4f}, \\tTrain Loss: {:.4f}, \\tTrain Accuracy: {:.2f} %, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], train_loss, train_accuracy, test_loss, test_accuracy))\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_accuracy)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_accuracy)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L58zUMido77I"
   },
   "outputs": [],
   "source": [
    "# What happens if dropout ratio is too high?\n",
    "\n",
    "class RegClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        self.dropout = Dropout(p=0.7)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.fc4(x),dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9AYy-Cpo-sP"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "model = RegClassifier().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "history = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[], 'lr':[]}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tLR: {:.4f}, \\tTrain Loss: {:.4f}, \\tTrain Accuracy: {:.2f} %, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], train_loss, train_accuracy, test_loss, test_accuracy))\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_accuracy)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_accuracy)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xllbIF_Cq9a9"
   },
   "source": [
    "# Add regularization to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1de81CRkq9H7"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hf1HUmmuk5a"
   },
   "outputs": [],
   "source": [
    "## How to apply early stopping?\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR as ExponentialLR\n",
    "\n",
    "EPOCHS = 10\n",
    "#### TODO ####\n",
    "\n",
    "##############\n",
    "history = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[], 'lr':[]}\n",
    "\n",
    "model = Model().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.01)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, scheduler)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tLR: {:.4f}, \\tTrain Loss: {:.4f}, \\tTrain Accuracy: {:.2f} %, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], train_loss, train_accuracy, test_loss, test_accuracy))\n",
    "\n",
    "    #### TODO ####\n",
    "\n",
    "    ##############\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_accuracy)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_accuracy)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbG0Ngi8WfXB"
   },
   "source": [
    "# Custom Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "It8MYuepW_b5"
   },
   "outputs": [],
   "source": [
    "## Implementing custom learning rate scheduler\n",
    "\n",
    "class ScheduledOptim():\n",
    "\n",
    "    def __init__(self, optimizer, n_warmup_steps, decay_rate, steps=None):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.decay = decay_rate\n",
    "        self.n_steps = 0\n",
    "        self.steps = steps\n",
    "        self.initial_lr = optimizer.param_groups[0]['lr']\n",
    "        self.current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.update()\n",
    "        # self._optimizer.step()\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.current_lr\n",
    "    \n",
    "    def update(self):\n",
    "        #### TODO ####\n",
    "\n",
    "        ##############\n",
    "\n",
    "        self.n_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtuMDR6u5CNT"
   },
   "outputs": [],
   "source": [
    "## How to apply customized learning rate \n",
    "\n",
    "EPOCHS = 10\n",
    "history = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[], 'lr':[]}\n",
    "\n",
    "model = Model().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#### TODO ####\n",
    "scheduler = ScheduledOptim()\n",
    "patience = 0 #patience : 위에서 implement다시하기!\n",
    "best_loss = 100\n",
    "scheduler.update()\n",
    "##############\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    lr = scheduler.get_lr()\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, scheduler)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tLR: {:.5f}, \\tTrain Loss: {:.4f}, \\tTrain Accuracy: {:.2f} %, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, lr, train_loss, train_accuracy, test_loss, test_accuracy))\n",
    "    \n",
    "    #### TODO ####\n",
    "    if test_loss < best_loss :\n",
    "        best_loss = test_loss\n",
    "    else:\n",
    "        patience += 1 \n",
    "        if patience >=2:\n",
    "            break\n",
    "    ##############\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_accuracy)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_accuracy)\n",
    "    history['lr'].append(lr)\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2Cl5_xRfsHQ"
   },
   "source": [
    "# RNN Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7pxGJbxi7dQ"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.datasets.AG_NEWS(root='./data')\n",
    "labels = [_, 'World', 'Sports', 'Business', 'Sci/Tech']\n",
    "y, x = next(iter(train_data))\n",
    "print(labels[y])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8B9GIWhK2yXx"
   },
   "outputs": [],
   "source": [
    "y, x = next(iter(test_data))\n",
    "print(labels[y])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kwGzxRV0V50"
   },
   "outputs": [],
   "source": [
    "set([label for (label, text) in train_data]), set([label for (label, text) in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2aMIZKi1OSl"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenizer(\"Hi, my name is Joonseok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwvIVvJ1_eS9"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "encoder = build_vocab_from_iterator(tokens(train_data), specials=[\"<unk>\"])\n",
    "encoder.set_default_index(encoder[\"<unk>\"])\n",
    "\n",
    "encoder(tokenizer(\"Hi, my name is Joonseok <unk> !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jokQ3-ieQe6h"
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: encoder(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y2TagVKoVLI"
   },
   "outputs": [],
   "source": [
    "iterator = iter(train_data)\n",
    "sample_batch = []\n",
    "for _ in range(8):\n",
    "    sample_batch.append(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ig0ue1ZE9F5F"
   },
   "outputs": [],
   "source": [
    "## What happens if we ignore zero-padding\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.stack(text_list).long()\n",
    "    return label_list, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAW87VyQ9Lj9"
   },
   "outputs": [],
   "source": [
    "collate_batch(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIGsKJ_3nbdJ"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 32\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        if processed_text.size(0) >= MAX_LEN:\n",
    "            processed_text = processed_text[:MAX_LEN]\n",
    "        else:\n",
    "            processed_text = torch.cat([processed_text, \n",
    "                                  torch.zeros(MAX_LEN-processed_text.size(0))])\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.stack(text_list).long()\n",
    "    return label_list, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsTFHdmH8kpC"
   },
   "outputs": [],
   "source": [
    "collate_batch(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1nZcLuTqX5r"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_data, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "y, x = next(iter(dataloader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_X4YQWhqcVQ"
   },
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_data]))\n",
    "vocab_size = len(encoder)\n",
    "emsize = 64\n",
    "hidden_dim = 32\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, emsize)\n",
    "rnn = nn.RNN(emsize, hidden_dim, 1, nonlinearity='tanh', bias=True, batch_first=True)\n",
    "fc = nn.Linear(hidden_dim, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNZZxxxEqrwZ"
   },
   "outputs": [],
   "source": [
    "## Check the tensor shapes after each layer\n",
    "\n",
    "init_h = torch.randn(1, 8, hidden_dim)\n",
    "output, hidden = rnn(embedding(x), init_h)\n",
    "# embedding(x).shape\n",
    "# fc(output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bjM39KY-KIP"
   },
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden, embed, num_class, batch_size):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed)\n",
    "        self.rnn = nn.RNN(input_size=embed, hidden_size=hidden, \n",
    "                          num_layers=1, nonlinearity='tanh', \n",
    "                          bias=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, h = self.rnn(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D15X40bEr7tA"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        label, text = label.to(DEVICE), text.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            label, text = label.to(DEVICE), text.to(DEVICE)\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZrD96hwsMiO"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameter Setup\n",
    "\n",
    "EPOCHS = 5\n",
    "LR = 1\n",
    "BATCH_SIZE = 64\n",
    "num_class = len(set([label for (label, text) in train_data]))\n",
    "vocab_size = len(encoder)\n",
    "emsize = 64\n",
    "hidden_dim = 32\n",
    "\n",
    "model = TextClassificationModel(vocab_size, hidden_dim, emsize, num_class, BATCH_SIZE).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_data)\n",
    "test_dataset = to_map_style_dataset(test_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXH3-AWYsUsx"
   },
   "outputs": [],
   "source": [
    "## Train the RNN for text classification\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrevX8E5VkSz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gJtxV0qQTldZ",
    "Ply4nFhlW2cB"
   ],
   "name": "lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
